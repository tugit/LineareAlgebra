% VO 26-04-2016 %
\section{Der Satz von Sylvester}
\paragraph{Beispiel}
	Ist $ \sigma $ symmetrische Sesquilinearform auf $ V=\mathbb{Z}^2_2 $ mit
		\[ \sigma(e_1,e_1):=0,\ \sigma(e_1,e_2) := 1,\ \sigma(e_2,e_2) := 0, \]
	so ist $ \sigma $ (wie vorher) nicht-degeneriert, $ V^\perp =\{0\}$; trotzdem gilt
		\[ \forall v\in V: \sigma(v,v) = 0. \]
	Das folgende Lemma zeigt, dass dies ein degenerierter Fall ist:
	
\subsection{Lemma \& Definition (Polarisation)}\index{quadratische Form}
\begin{Lemma}[Polarisationslemma]	
	Ist $ \sigma $ symmetrische Bilinearform auf einem $ K $-VR $ V $ über einem Körper $ K $ mit $ \Char K \neq 2 $, so gilt
		\[ \forall v,w\in V: \sigma(v,w)=\frac{1}{2}\left(q(v+w)-q(v)-q(w)\right), \]
	wobei
\end{Lemma}
\begin{Definition}[quadratische Form]
		\[ q:V\to K,\ v\mapsto q(v):= \sigma(v,v) \]
	die zu $ \sigma $ gehörige \emph{quadratische Form} bezeichnet.
\end{Definition}
\paragraph{Beweis}
	Ausrechnen: sind $ v,w\in V $, so gilt
	\begin{align*}
	q(v+w) &= \sigma(v+w,v+w)\\
			&= \sigma(v,v) + \sigma(v,w)+\sigma(w,v)+\sigma(w,w)\\
			&= q(v)+2\sigma(v,w)+q(w)
	\end{align*}
	diese Gleichung kann (da $ \Char K\neq 2 $) nach $ \sigma(v,w) $ aufgelöst werden.
\paragraph{Bemerkung}
	Ist $ \Char K=0 $ so kann man statt
		\[ q(v+w)=q(v)+2\sigma(v,w)+q(w) \]
	auch
		\[ q(v+w)-q(v-w) = 4 \sigma(v,w) \]
	für die Polarisation verwenden.
	
\subsection{Lemma}
\begin{Lemma}[]
	Ist $ \sigma $ symmetrische Sesquilinearform auf einem $ K $-VR $ V $ über einem Körper $ K $ mit $ \Char K \neq 2 $, so gilt
		\[ \sigma = 0 \Leftrightarrow \forall v\in V: \sigma(v,v) = 0. \]
\end{Lemma}
\paragraph{Bemerkung}
	Im Falle einer Bilinearform folgt dies direkt mit Polarisation.
	
	Im Falle eines nicht-trivialen Körperautomorphismus $ \bar{.} $ liefert $ v\mapsto \sigma(v,v) $ wegen
		\[ K\ni x \mapsto \sigma(vx,vx)-x^2\sigma(v,v) = (\overline{x}x-x^2)\sigma(v,v)\neq 0 \]
	im Allgemeinen \emph{keine} quadratische Form:
		\[ \exists x\in K: \exists v\in V: \sigma(vx,vx) = \overline{x}\sigma(v,v)x \neq x^2\sigma(v,v). \]
\paragraph{Beweis}
	Ist $ \sigma = 0 $, so folgt trivialerweise
		\[ \forall v\in V: \sigma(v,v) = 0. \]
	Sei nun $ \sigma \neq 0 $, d.h.
		\[ \exists v,w\in V: \sigma(v,w)\neq 0. \]
	Wie vorher berechnet man für $ v,w\in V $
		\[ \sigma(v+w,v+w) = \sigma(v,v)+\sigma(v,w)+\overline{\sigma(v,w)}+\sigma(w,w). \]
	Wähle nun $ v,w\in V $ mit $ \sigma(v,w)\neq 0 $, o.B.d.A. $ \sigma(v,w) = 1 $.
	\footnote{Ggf. ersetzt man $ w $ durch $ \frac{w}{\sigma(v,w)} $.}
	Ist $ \sigma(v,v) \neq 0 $ oder $ \sigma(w,w)\neq 0 $, so sind wir fertig.
	
	Gilt jedoch $ \sigma(v,v) = \sigma(w,w) = 0 $, so liefert
		\[ \sigma(v+w,v+w) = 0 + 1 + 1 + 0 \neq 0 \]
	wieder die Behauptung, da $ \Char K \neq 2 $.
\paragraph{Vereinbarung}
	Im Folgenden schließen wir $ \Char K = 2 $ aus.
	
\subsection{Lemma}
\begin{Lemma}[]
	Für eine symmetrische Sesquilinearform $ \sigma $ auf $ V $ und $ b\in V $ mit $ \sigma(b,b)\neq 0 $ gilt
		\[ V = [b]\oplus \{b\}^\perp. \]
\end{Lemma}
\paragraph{Beweis}
	Es gilt $ V = [b]+\{b\}^\perp $, da für $ v\in V $
		\[ v = u + b\frac{\sigma(b,v)}{\sigma(b,b)} \text{ mit } u := v-b\frac{\sigma(b,v)}{\sigma(b,b)}\perp b;\footnote{denn $ \sigma(b,u) = \sigma(b,v-b\frac{\sigma(b,v)}{\sigma(b,b)}) = \sigma(b,v)-\sigma(b,b)\frac{\sigma(b,v)}{\sigma(b,b)} = 0 $} \]
	Ist außerdem $v=bx$, also $ v\in [b]\cap \{b\}^\perp $, so gilt
		\[ 0 = \sigma(b,v) = \sigma(b,bx) = \underbrace{\sigma(b,b)}_{\neq 0}x \]
			\[ \Rightarrow x = 0 \Rightarrow v = 0, \]
	d.h. $ [b]\cap \{b\}^\perp = \{0\}$ und damit folgt die Behauptung.
\paragraph{Bemerkung}
	Ist $ \sigma(b,b) = 0 $ für $ b \in V $, so gilt
		\[ b\in [b]\cap \{b\}^\perp, \]
	d.h. ist $ b\neq 0 $, so ist $ [b]\cap \{b\}^\perp \neq \{0\}$. Außerdem ist dann $ \sigma\big|_{U\times U} $ für $ U:= \{b\}^\perp $ degeneriert, da
		\[ \exists v = b \in U^\times \forall u\in U: u\perp b.\]
		
\subsection{Diagonalisierungslemma}
\begin{Lemma}[Diagonalisierungslemma]
	Zu jeder symmetrischen Sesquilinearform $ \sigma $ auf einem endlichdimensionalen VR $ V $, also $ n = \dim V < \infty $,  gibt es eine Basis $ B = (b_1,\dots,b_n) $ von $ V $, die $ \sigma $ \emph{diagonalisiert}, d.h. für die gilt
		\[ \sigma(b_i,b_j)=0, \text{ falls } i\neq j. \]
\end{Lemma}

\paragraph{Beweis}
	Durch Induktion über $ n $.
	
	Für $ n = 1 $ ist die Behauptung trivial (denn $ i\neq j $ existiert nicht).
	
	Sei die Behauptung also für $\dim V = n $ bewiesen. Ist $ \sigma $ symmetrische Sesquilinearform auf $ V $ mit $ \dim V = n+1 $ und o.B.d.A. $ \sigma \neq 0 $, also
		\[ \exists b\in V: \sigma(b,b) \neq 0 \]
	nach obigem Lemma lässt sich also $ V $ aufspalten in 
		\[ V = [b]\oplus U \text{ mit } U:=\{b\}^\perp \]
	und $ \dim U = n $. Nach Annahme existiert eine Basis $ (b_1,\dots,b_n) $ von $ U $, die $ \sigma|_{U\times U} $ diagonalisiert. Da $ b \perp b_1,\dots,b_n \in U$ liefert $ B := (b,b_1,\dots,b_n) $ eine $ \sigma $-diagonalisierende Basis von $ V $. 
\paragraph{Bemerkung}
	Ist $ B = (b_1,\dots,b_n) $ eine $ \sigma $-diagonalisierende Basis, also
		\[ s_{ij} = \sigma(b_i,b_j) = 0 \text{ für } i\neq j \]
	so ist
		\[ \sigma(v,v) = \sum_{i=1}^{n}\overline{x_i}s_{ii}x_i \text{ für } v = \sum_{i=1}^{n}b_ix_i. \]
	Sind $ a_1,\dots,a_n\in K^\times $ und $ b_i' = b_ia_i $, so zeigt
		\[ s_{ij}' = \sigma(b_i',b_j') = \overline{a_i}\sigma(b_i,b_j)a_j = \overline{a_i}s_{ij}a_j, \]
	dass $ B' = (b_1',\dots,b_n') $ eine weitere $ \sigma $-diagonalisierende Basis ist.
	Man kann also die $ s_{ii} $ "`adjustieren"', sofern man die (unabhängigen) Gleichungen
		\[ s_{ii}' = \overline{a_i}s_{ii}a_i \]
	für gegebene $ s_{ii}' $ (nach den $ a_i $) lösen kann. Zum Beispiel:
\subsection{Korollar}
\begin{Korollar}[]\label{silkor1}
	Ist $ \sigma $ symmetrische Bilinearform auf einem $ \C $-VR $ V $ mit $ \dim V < \infty $, so besitzt $ V $ eine Basis $ B = (b_1,\dots,b_n) $, sodass
		\[ \exists r\in \mathbb{N}: s_{ij} = \sigma(b_i,b_j) = \begin{cases}
		1 & \text{für } i = j \leq r\\
		0 & \text{sonst}.
		\end{cases} \]
\end{Korollar}

% VO 28-04-2016 %
\paragraph{Bemerkung}
	D.h.
		\[ \Gamma_B(\sigma) = \begin{pmatrix}
		E_r & 0 \\ 0 & 0
		\end{pmatrix}. \]
\paragraph{Beweis}
	Sei (nach Diagonalisierungslemma) $ B' = (b_1',\dots,b_n') $ eine $ \sigma$-diagonalisierende Basis von $ V $ -- durch Umsortierung der Basisvektoren kann man erreichen, dass
		\[ s_{11}' ,\dots, s_{rr}' \neq 0 \text{ und } s_{r+1,r+1}' = \dots = s_{nn}' = 0 \]
	für ein $ r\in \{0,\dots,n\} $. Mit einer Wahl der Wurzel bilden die Vektoren 
		\[ b_i := \begin{cases}
		{b_i'}\cdot \frac{1}{\sqrt{s_{ii}'}} & \text{ für } i = 1,\dots,r\\
		b_i' = 0 & \text{ für } i = r+1,\dots,n 
		\end{cases} \]
	dann eine Basis $ B $ mit der gewünschten Eigenschaft:
		\[ \sigma(b_i,b_i) = \underset{s_{ii}'}{\underbrace{\sigma(b_i',b_i')}} \cdot \left(\frac{1}{\sqrt{s_{ii}'}}\right)^2 = 1 \text{ für } i = 1,\dots,r\]
		\[ \sigma(b_i,b_i) = \sigma(b_i',b_i') = 0 \text{ für } i = r+1,\dots, n. \]
		
\subsection{Korollar}
\begin{Korollar}[]\label{silkor2}
	Ist $ V $ ein $ K $-VR mit $ \dim V <\infty $ und $ \sigma $ entweder
		\begin{itemize}
			\item symmetrische Bilinearform, wenn $ K=\R $, oder
			\item Hermitesche Sesquilinearform, wenn $ K = \C $,
		\end{itemize}
	so besitzt $ V $ eine Basis $ B = (b_1,\dots,b_n) $, sodass
		\[ \exists r\in \mathbb{N}: s_{ij} = \sigma(b_i,b_j) =
		\begin{cases}
			\pm 1 & \text{ für }i = j \leq r\\
			0 & \text{ sonst.}
		\end{cases} \]
\end{Korollar}
\paragraph{Beweis}
	Wie oben -- aber:
	In diesen beiden Fällen gilt für eine diagonalisierende Basis $ B'=(b_1',\dots,b_n') $ und $ b_i = b_i'\cdot \frac{1}{a_i} $ mit $ a_i\in K $ für $ i=1,\dots,n $:
		\[ s_{ii}' = \sigma(b_i',b_i')\in \R \text{ und }
		\begin{cases}
			a_i^2 \geq 0 & \text{falls } K = \R,\\
			\overline{a_i}a_i \geq 0 & \text{falls } K =\C.
		\end{cases} \]
	Also kann man die $ s_{ii}' $ (nur) positiv reskalieren und so $ s_{ii} = 0 $ oder $ s_{ii} = \pm 1 $ erreichen.
\paragraph{Notation}
	Im Folgenden bezeichnet $ \K $ entweder $ \R $ oder $ \C $.
\paragraph{Motivation}
	Für die obige Basis $ B $ von $ V $ mit den Eigenschaften des Korollars gilt offenbar:
		\[ v\perp b_1,\dots,b_r \Rightarrow v\in [\{b_{r+1},\dots,b_{n}\}] \]
	und
		\[ b_{r+1},\dots,b_n \perp V, \]
	also ist $ (b_{r+1},\dots,b_n) $ Basis des Radikalraums $ V^\perp $ von $ (V,\sigma) $,
		\[ V^\perp = [\{b_{r+1},\dots,b_n\} ] \Rightarrow r = \dim V-\dim V^\perp. \]
	Insbesondere ist $ \dim V^\perp $ und damit $ r $ unabhängig von der Basis $ B $.

\subsection{Satz von Sylvester}\index{Sesquilinearform!Signatur}
\begin{Satz}[Satz von Sylvester (Trägheitssatz von Sylvester)]	
	Sei $ V $ ein $ \K $-VR, $ \dim V <\infty $, und $ \sigma $
		\begin{itemize}
			\item symmetrische Bilinearform, wenn $ \K=\R $, oder
			\item Hermitesche Sesquilinearform, wenn $ \K=\C $.
		\end{itemize}
	Dann gibt es eine direkte Zerlegung von $ V $ mit UVR $ V_{\pm}\subset V $,
		\[ V= V_+ \oplus_\perp V_- \oplus_\perp V^\perp,  \]
	wobei
		\[ V_+ \perp V_- \text{ und } \forall v\in V^\times_\pm: \pm \sigma(v,v) > 0. \]
\end{Satz}
\begin{Definition}[Signatur]
	Die \emph{Signatur} $ \sgn(\sigma):= (\dim V_+,\dim V_-,\dim V^\perp) $ von $ \sigma $ ist unabhängig von der direkten Zerlegung von $ V $.
\end{Definition}
\paragraph{Bemerkung \& Definition}\index{Trägheitsindex}\index{Positivitätsindex}\index{Negativitätsindex}
\begin{Definition}[Signatur, Trägheitsindex,Positivitäts- ,Negativitätsindex  ]
	Ist $ \sigma $ nicht-degeneriert, $ V^\perp = \{0\} $, so bezeichnet man auch\footnote{Die Reihenfolge kann bei verschiedenen Autoren auch jeweils $ - $ vor $ + $ sein.}
		\begin{itemize}
			\item das Paar $ \sgn (\sigma) =(\dim V_+,\dim V_-)$ als Signatur von $ \sigma $, und
			\item die Differenz $ \dim V_+ - \dim V_- $ als \emph{Trägheitsindex} von $ \sigma $.
		\end{itemize}
	Die Dimension $ \dim V_\pm $ ist auch der \emph{Positivitäts-} bzw. \emph{Negativitätsindex} von $ \sigma $.
\end{Definition}	
	Der Satz von Sylvester wird auch "`Trägheitssatz von Sylvester"' genannt.
\paragraph{Beweis}
	Sei $ B=(b_1,\dots,b_n) $ eine Basis von $ V $ und $ p,r\in \mathbb{N} $, sodass (siehe Korollar \ref{silkor2})
		\[ \sigma(b_i,b_j) =
		\begin{cases}
			+1 & \text{ für } 0 < i=j\leq p\\
			-1 & \text{ für } p < i=j\leq r\\
			0 & \text{ sonst. }
		\end{cases} \]
	Mit
		\[ V_+ := [\{b_1,\dots,b_p \}] \text{ und } V_- := [\{b_{p+1},\dots,b_r\}] \]
	erhält man die gewünschte direkte orthogonale Zerlegung von $ V $,
		\[ V = V_+ \oplus_\perp V_- \oplus_\perp V^\perp. \]
	Zur Eindeutigkeit der Signatur $ \sgn(\sigma) = (p,r-p,n-r) $:
	
	Seien
	\[ V=V_+ \oplus_\perp V_- \oplus_\perp V^\perp
	= \tilde{V}_+\oplus_\perp\tilde{V}_-\oplus_\perp\tilde{V}^\perp \]
	direkte orthogonale Zerlegungen von $ V $ mit
		\[ \pm \sigma(v,v)>0 \text{ für }
			\begin{cases}
				v\in V_\pm^\times\\
				v\in \tilde{V}_\pm^\times.
			\end{cases} \]
	Nun gilt
	\begin{gather*}
		\forall v\in V_-^\times: \sigma(v,v)< 0 \\
		\Rightarrow \forall v\in V_- \oplus V^\perp: \sigma(v,v) \leq 0
	\end{gather*}
	und damit, da $ \sigma(v,v)>0 $ für $ v\in \tilde{V}_+^\times $,
		\[ v\in (V_-\oplus V^\perp)\cap \tilde{V}_+ \Rightarrow v= 0. \]
	Es folgt, mit dem Dimensionssatz, $ \tilde{p}\leq p $, da
		\[ \tilde{p}+(n-p) = \dim \tilde{V}_+ + \dim(V_-\oplus V^\perp) \leq \dim V = n. \]
	Vertauscht man die Rollen der Zerlegungen, so erhält man die Ungleichung $ p\leq \tilde{p} $ und damit also
		\[ p = \tilde{p}. \]
\paragraph{Bemerkung}
	Diese Zerlegung $ V= V_+ \oplus_\perp V_- \oplus_\perp V^\perp $ ist im Allgemeinen \emph{nicht} eindeutig!
\paragraph{Beispiel}
	Betrachte eine durch ihre Werte auf der Standardbasis $ E=(e_1,e_2) $ gegebene symmetrische Bilinearform $ \sigma: \R^2\times \R^2 \to \R $.
		\begin{enumerate}
			\item $ S=(\sigma(e_i,e_j))_{i,j\in \{1,2\}} =
			\begin{pmatrix}
				0&1\\1& 0
			\end{pmatrix} $. Mit $ P:=\begin{pmatrix}
			1&1\\ 1& -1
			\end{pmatrix}\in Gl(2) $ liefert ein Basiswechsel $ B=EP $
				\[ (\sigma(b_i,b_j))_{i,j\in \{1,2\}} = P^tSP = \begin{pmatrix}
				2 & 0 \\ 0 & -2
				\end{pmatrix} \]
			die Signatur $ \sgn(\sigma) = (1,1,0)\cong (1,1) $.
			Jeder weitere Basiswechsel
				\[ \tilde{B}=BQ \quad\text{mit}\quad Q = \begin{pmatrix}
				\cosh(s) & \sinh(s)\\ \sinh(s)& \cosh(s) 
				\end{pmatrix}, s\in \R, \]
			liefert eine andere Zerlegung, ohne die Gramsche Matrix zu ändern.
			\item $ S=(\sigma(e_i,e_j))_{i,j\in \{1,2\}}= \begin{pmatrix}
			1 & 1\\ 1 & 1
			\end{pmatrix}. $ Der Basiswechsel $ B=EP $ wie oben liefert hier
				\[ (\sigma(b_i,b_j))_{i,j\in \{1,2\}} = P^tSP = \begin{pmatrix}
				4 & 0 \\ 0 & 0
				\end{pmatrix}, \]
			also die Signatur $ \sgn(\sigma) = (1,0,1) $ von $ \sigma $. Hier ist $ V^\perp = [\{b_2\}]$ durch $ \sigma $ festgelegt, aber jeder Basiswechsel
				\[ \tilde{B} = BQ \quad\text{mit}\quad Q= \begin{pmatrix}
				1 & 0 \\ s & 1
				\end{pmatrix}, s\in \R \]
			ändert die der Basis zugeordnete Zerlegung -- wieder ohne Änderung der Gramschen Matrix.
		\end{enumerate}

% VO 03-05-2016 %
\subsection{Bemerkung \& Definition}\index{Äquivalenz von Sesquilinearformen}
	Zur geometrischen Analyse der Lösungsmengen von quadratischen Gleichungen (Quadriken), ist es hilfreich, eine \emph{Äquivalenz} für symmetrische Bilinearformen/Sesquilinearformen $ \sigma $ und $ \sigma' $ auf einem $ \K $-VR $ V $ einzuführen:
		\[ \sigma' \sim \sigma :\Leftrightarrow \exists f\in Gl(V)\forall v,w\in V: \sigma'(v,w) = \sigma(f(v),f(w)). \]
	Ist $ \dim V <\infty $, so liefert der Satz von Sylvester im Falle
		\begin{itemize}
			\item symmetrische Bilinearform auf $ \R $-VR, oder
			\item Hermitesche Sesquilinearform auf $ \C $-VR:
		\end{itemize}
\paragraph{Satz:}
	Zwei symmetrische Sesquilinearformen sind genau dann äquivalent, wenn ihre Signaturen übereinstimmen,
		\[ \sigma' \sim \sigma \Leftrightarrow  \sgn(\sigma') = \sgn(\sigma). \]
		
\subsection{Definition}\index{Skalarprodukt}\index{Orthonormal!-system}\index{Orthonormal!-basis}
	Ein \emph{Skalarprodukt} auf einem $ K $-VR $ V $ ist eine nicht-degenerierte symmetrische Sesquilinearform
		\[ \langle.,.\rangle:V\times V \to K,\ (v,w)\mapsto \langle v,w\rangle. \]
	Eine Familie $ (e_i)_{i\in I} $ in einem VR $ (V,\langle.,.\rangle) $ mit Skalarprodukt heißt \emph{Orthonormalsystem (ONS)}, falls
		\[ \forall i,j\in I: \langle e_i,e_j\rangle = \pm \delta_{ij}; \]
	\emph{Orthonormalbasis (ONB)}, falls $ (e_i)_{i\in I} $ zusätzlich Basis ist.
\paragraph{Bemerkung}
	Ein ONS ist linear unabhängig:

	Für $ v=\sum_{i\in I}e_ix_i $ gilt
		\[ 0 = v \quad\Rightarrow \forall i\in I: 0 = \langle e_i,v\rangle = \sum_{j\in I}\langle e_i,e_j\rangle x_j = \pm x_i \]
	Ist $ \dim V < \infty $, so hat $ (V,\langle.,.\rangle) $ jedenfalls eine ONB, wenn das Skalarprodukt (siehe \ref{silkor1} und \ref{silkor2})
		\begin{itemize}
			\item symmetrische Bilinearform auf einem $ \K $-VR ist, oder
			\item Hermitesche Sesquilinearform auf einem $ \C $-VR ist.
		\end{itemize}
	Ist $ K\neq \K $, so kann die "`Normierung"' problematisch sein.
\paragraph{Beispiel}
	Auf dem $ \R $-VR der beschränkten Zahlenfolgen:
		\[ V=\{(x_n)_{n\in \mathbb{N}}\in \R^\mathbb{N}, \exists c\in \R\forall n\in \mathbb{N}: |x_n|<c \}, \]
	führen wir ein Skalarprodukt, durch Angabe seiner quadratischen Form (Polarisation!) ein:
		\[ \langle (x_n)_{n\in \mathbb{N}},(x_n)_{n\in \mathbb{N}}\rangle := \sum_{n\in \mathbb{N}}\left(\frac{x_n}{2^n}\right)^2.  \]
	%PERSONAL ADDITION:
		Mittels Polarisation ist für $(x_n)_{n \in \N},(y_n)_{n \in \N} \in V$ also
			\[\sigma((x_n)_{n \in \N},(y_n)_{n \in \N})=\sum_{n \in \N} \frac{x_ny_n}{2^{2n}}\]
	Man erhält ein ONS $ (e_n)_{n\in \mathbb{N}} $ aus skalierten Standardvektoren
		\[ e_m:\mathbb{N}\to \R,\ n\mapsto e_m(n):= 2^n\delta_{mn}. \]
	Dieses ONS kann zu einer Basis ergänzt werden (nach BES), nicht jedoch zu einer ONB (in unserem Sinne)\footnote{$v$ bezeichnet einen Vektor mit dem wir unser ONS zu erweitern versuchen, mit der Anforderung, dass die Erweiterung durch $v$ wieder ein ONS ist!}:
		\[ \langle e_m, v\rangle = \frac{x_m}{2^m} \quad\text{für } v=(x_{n})_{n\in \mathbb{N}}, \]
	also gilt die Implikation
		\[ \forall m\in \mathbb{N}: v\perp e_m \Rightarrow v = 0. \]
\paragraph{Bemerkung}
	Später wird der Begriff "`Basis"' modifiziert, z.B. in der Funktionalanalysis würde man $ (e_m)_{m\in \mathbb{N}} $ aus dem Beispiel als "`Orthonormalbasis"' bezeichnen.