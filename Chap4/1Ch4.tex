\chapter{Volumenmessung}
	Grundlegende Idee: Wir definieren ein Spat- oder Parallelotop-Volumen.

	Algebraisch: Dieses Volumen kann dann benutzt werden, um zu testen, wann ein Spat/Pa"-ral"-lelotop "`zusammenklappt"'.
\section{Determinantenformen}
	Idee: Für den Flächeninhalt $ F(v,w) $ eines von zwei Vektoren $ v,w\in V $ aufgespannten Parallelogramms gilt
	\begin{gather*}
		F{(vx,w)} = F{(v,w)}\cdot x\\
		F(v+v',w) = F(v,w)+F(v',w)
	\end{gather*}
	und entsprechend für das zweite Argument; außerdem verschwindet der Flächeninhalt, wenn das Parallelogramm "`zusammenklappt"', also insbesondere gilt
		\[ w=v\Rightarrow F(v,w)=0 \]
	Die folgende Definition verallgemeinert diese Eigenschaften:

\definecolor{ttttff}{rgb}{0.2,0.2,1}
\definecolor{ttfftt}{rgb}{0.2,1,0.2}
\definecolor{uququq}{rgb}{0.25,0.25,0.25}
\definecolor{qqqqff}{rgb}{0,0,1}

%-------------------Begin Addition mit einem Vektor ----------------  
\begin{figure}[H]\centering
\tdplotsetmaincoords{0}{0} %-27
\begin{tikzpicture}[yscale=1,tdplot_main_coords]

\def\xstart{0} %x Koordinate der Startposition der Grafik
\def\ystart{0} %y Koordinate der Startposition der Grafik
\def\myscale{0.9} %ändert die Größe der Grafik (Skalierung der Grafik) 

\def\xstartdraw{(\xstart + 1.5)} %xKoordinate des Referenzstartpunktes (in dieser Zeichnung: a)
\def\ystartdraw{(\ystart + 3.5)}%yKoordinate des Referenzstartpunktes (in dieser Zeichnung: a)

\def\balkenhoehe{(5.3)}% Länge des vertikalen blauen Balkens
\def\balkenlaenge{(10)}% Länge des horizontalen blauen Balkens
\def\balkenbreite{0.4} %Balkenbreite

%---------Begin Balken----------
\def\drehwinkel{0}
\node (VekV) at ({\xstart+0.7*cos(\drehwinkel)-\balkenbreite*sin(\drehwinkel)},{\ystart+0.5*sin(\drehwinkel)+\balkenbreite*cos(\drehwinkel)})[right, xshift=1,color=blue] {$V$};
\node (AffA) at ({\xstart+(\balkenlaenge-1)*cos(\drehwinkel)},{\ystart+(\balkenlaenge-1)*sin(\drehwinkel)+\balkenbreite*cos(\drehwinkel)})[color=red] {$A^2$};

\path[ shade, top color=white, bottom color=blue, opacity=.6] 
    ({\xstart},{\ystart},0)  -- ({\xstart - \balkenbreite * cos(\drehwinkel)- (-\balkenbreite+0)*sin(\drehwinkel)},{\ystart - \balkenbreite * sin(\drehwinkel)+ (-\balkenbreite+0)*cos(\drehwinkel)},0)  -- ({\xstart - \balkenbreite * cos(\drehwinkel)- (\balkenhoehe+0.5)*sin(\drehwinkel)},{\ystart - \balkenbreite * sin(\drehwinkel)+ (\balkenhoehe+0.5)*cos(\drehwinkel)},0) -- ({\xstart - 0 * cos(\drehwinkel)- (\balkenhoehe+0)*sin(\drehwinkel)},{\ystart - 0 * sin(\drehwinkel)+ (\balkenhoehe+0)*cos(\drehwinkel)},0) -- cycle;
        
\path[ shade, right color=white, left color=blue, opacity=.6] 
	({\xstart},{\ystart},0)  -- ({\xstart - \balkenbreite * cos(\drehwinkel)- (-\balkenbreite+0)*sin(\drehwinkel)},{\ystart - \balkenbreite * sin(\drehwinkel)+ (-\balkenbreite+0)*cos(\drehwinkel)},0) --
    ({\xstart + (\balkenlaenge+0.5) * cos(\drehwinkel)- (-\balkenbreite+0)*sin(\drehwinkel)},{\ystart + (\balkenlaenge+0.5) * sin(\drehwinkel)+ (-\balkenbreite+0)*cos(\drehwinkel)},0) --   
    ({\xstart + \balkenlaenge * cos(\drehwinkel)},{\ystart + \balkenlaenge * sin(\drehwinkel)},0)--
    cycle;       
%---------End Balken----------
\def\lightoffset{0.2*\myscale} %offeset der Vektoren

%Punkte Definition
\node (pointa1) at ({\xstartdraw},{\ystartdraw}) {};
\node (pointa2) at ({\xstartdraw+(1 *\myscale)},{\ystartdraw-(2.0*\myscale)}) {};
\node (pointb1) at ($(pointa1) + (3.0*\myscale,-1.0*\myscale) $) {};
\node (pointb2) at ($(pointb1) + (1.0*\myscale,-2.0*\myscale) $) {};

\node (pointc1) at ($(pointa1) + (6.5*\myscale,1.3*\myscale) $) {};
\node (pointc2) at ($(pointa2) + (6.5*\myscale,1.3*\myscale) $) {};

\node (pointFvwi) at ($(pointb2) + (-0.9*\myscale,0.9*\myscale) $) {};
\node (pointFvwa) at ($(pointb2) + (-1.5*\myscale,-0.3*\myscale) $) {};

\node (pointFvswi) at ($(pointc2) + (-2.9*\myscale,-1.2*\myscale) $) {};
\node (pointFvswa) at ($(pointc2) + (-1.3*\myscale,-1.9*\myscale) $) {};

\node (pointfgi) at ($(pointa1) + (2.2*\myscale,-0.2*\myscale) $) {};
\node (pointfga) at ($(pointfgi) + (-1.4*\myscale,1.8*\myscale) $) {};



%Flächen füllen
%blaue Flaeche
\fill[color=ttttff,fill=ttttff,fill opacity=0.15] (pointa1.center) -- (pointb1.center) -- (pointc1.center) -- (pointc2.center) -- (pointb2.center)-- (pointa2.center)-- cycle;
%gruene Flaeche
\fill[color=ttfftt,fill=ttfftt,fill opacity=0.5] (pointa1.center) -- (pointc1.center) -- (pointc2.center) -- (pointa2.center)-- cycle;

%Vektoren blau
\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointa1) -- (pointb1);
\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointa2) -- (pointb2);
\node [color=blue] (pointlabelg1) at ($(pointa1)!0.5!(pointb1)$) [above, xshift=0, yshift=0] {$v$} ;
\node [color=blue] (pointlabelg2) at ($(pointa2)!0.5!(pointb2)$) [above, xshift=0, yshift=0] {$v$} ;

\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointb1) -- (pointc1);
\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointb2) -- (pointc2);
\node [color=blue] (pointlabelg3) at ($(pointb1)!0.5!(pointc1)$) [above, xshift=0, yshift=0] {$v'$} ;
\node [color=blue] (pointlabelg4) at ($(pointb2)!0.5!(pointc2)$) [above, xshift=0, yshift=0] {$v'$} ;

\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointa2) -- (pointa1);
\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointb2) -- (pointb1);
\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointc2) -- (pointc1);

\node [color=blue] (pointlabelga2a1) at ($(pointa2)!0.5!(pointa1)$) [left, xshift=0, yshift=0] {$w$} ;
\node [color=blue] (pointlabelgb2b1) at ($(pointb2)!0.5!(pointb1)$) [right, xshift=0, yshift=0] {$w$} ;
\node [color=blue] (pointlabelgc2c1) at ($(pointc2)!0.5!(pointc1)$) [right, xshift=0, yshift=0] {$w$} ;

%Vektoren gruen
\draw[-{>[scale=1,length=10,width=6]},shorten >=4pt, shorten <=4pt,line width=0.2pt,color=green] (pointa1) -- (pointc1);
\draw[-{>[scale=1,length=10,width=6]},shorten >=4pt, shorten <=4pt,line width=0.2pt,color=green] (pointa2) -- (pointc2);
\node [color=green] (pointlabelac1) at ($(pointa1)!0.5!(pointc1)$) [above, xshift=0, yshift=0] {$v+v'$} ;
\node [color=green] (pointlabelac2) at ($(pointa2)!0.5!(pointc2)$) [below, xshift=10, yshift=5] {$v+v'$} ;

%Punkte malen
\draw[fill,color=red] (pointa1) circle [x=1cm,y=1cm,radius=0.08]node[above, xshift=0, yshift=0]{};
\draw[fill,color=red] (pointb1) circle [x=1cm,y=1cm,radius=0.08]node[above, xshift=0, yshift=0]{};
\draw[fill,color=red] (pointa2) circle [x=1cm,y=1cm,radius=0.08]node[below, xshift=5, yshift=0]{};
\draw[fill,color=red] (pointb2) circle [x=1cm,y=1cm,radius=0.08]node[below, xshift=5, yshift=0]{};
\draw[fill,color=red] (pointc1) circle [x=1cm,y=1cm,radius=0.08]node[below, xshift=5, yshift=0]{};
\draw[fill,color=red] (pointc2) circle [x=1cm,y=1cm,radius=0.08]node[below, xshift=5, yshift=0]{};

\draw[->,shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointFvwa) -- (pointFvwi);
\draw[->,shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointFvswa) -- (pointFvswi);
\draw[->,shorten >=2pt, shorten <=2pt,line width=0.2pt,color=green] (pointfga) -- (pointfgi);

\node [color=blue] (pointlabelFvwl) at (pointFvwa) [xshift=0.5, yshift=-0.5] {$F(v,w)$} ;
\node [color=blue] (pointlabelFvswl) at (pointFvswa) [xshift=-0.5,  yshift=-5] {$F(v',w)$} ;
\node [color=green] (pointlabelFvswl) at (pointfga) [xshift=35 ] {$F(v+v',w)=\textcolor{blue}{F(v,w)+F(v',w)}$} ;

\end{tikzpicture}
\end{figure}
%-------------------End Addition mit einem Vektor ----------------

\subsection{Definition}
	\begin{Definition}[Linearform/Determinantenform]
	Sei $ V $ ein $ K $-VR. Eine Abbildung $ \omega:V^m\to K $ heißt
		\begin{itemize}
		\item \emph{$ m $-linear}, bzw. eine \emph{$ m $-(Linear-)Form}, falls $ \omega $ in jedem Argument linear ist, d.h.
			\[ \forall i=1,\dots, m: V\ni v_i\mapsto \omega(v_1,\dots,v_{i-1},v_i,v_{i+1},\dots,v_m)\in K \]
			ist linear;
		\item \emph{alternierend}, falls $ \omega(v_1,\dots,v_m)=0 $ wann immer zwei Vektoren gleich sind, d.h.
			\[ v_i = v_j \text{ für } i\neq j \Rightarrow \omega(v_1,\dots, v_m) = 0. \] 
		\end{itemize}
	Die Menge der alternierenden $ m $-Formen wird mit $ \Lambda^mV^* $ bezeichnet. Ist $ \dim V = n $, so heißt ein $ \omega\in \Lambda^nV^* $ auch Determinantenform.
	\end{Definition}

\paragraph{Beispiel}
	Jede Linearform $ \omega\in V^* $ ist eine (alternierende) 1-Form, $ \Lambda^1V^*=V^* $.
\paragraph{Bemerkung}
	$ \Lambda^mV^* $ ist für jedes $ m\in \mathbb{N} $ selbst ein $ K $-VR. 
\subsection{Lemma}
	\begin{Lemma}
		Für eine alternierende $ m $-Form $ \omega \in \Lambda^mV^* $ und $ i\neq j $ gilt:
		\begin{enumerate}[(i)]
			\item $ \omega(\dots,v_i,\dots,v_j,\dots) = -\omega (\dots,v_j,\dots,v_i,\dots)$;
			\item $ \omega(\dots,v_i,\dots,v_is+v_j,\dots) = \omega(\dots,v_i,\dots,v_j,\dots) $ für $ s\in K $;
			\item und $ \omega(v_1,\dots,v_m)=0 $, falls $ (v_i)_{i\in \{1,\dots,m\}} $ linear abhängig ist.
		\end{enumerate}
\paragraph{Beweis}
	Seien $ v_1,\dots,v_m\in V $ und $ i,j\in \{1,\dots,m\} $ mit $ i\neq j $. Dann gilt:
		\begin{align*}
			0 &= \omega(\dots,v_i+v_j,\dots,v_i+v_j,\dots)\\
			&= \omega(\dots,v_i,\dots,v_i,\dots)+\omega(\dots,v_j,\dots,v_j,\dots)+\omega(\dots,v_i,\dots,v_j,\dots)+\omega(\dots,v_j,\dots,v_i,\dots)\\
			&= \omega(\dots,v_i,\dots,v_j,\dots)+\omega(\dots,v_j,\dots,v_i,\dots)
		\end{align*}
	und
		\begin{align*}
		0&=\omega(\dots,v_i,\dots,v_is,\dots)\\
		&= \omega(\dots,v_i,\dots,v_is+v_j-v_j,\dots)\\
		&= \omega(\dots,v_i,\dots,v_is+v_j,\dots)-\omega(\dots,v_i,\dots,v_j,\dots)
		\end{align*}
	Dies beweist (i) und (ii). Ist die Familie $ (v_i)_{i\in \{1,\dots,m\}} $ linear abhängig, o.B.d.A
		\[ v_m = \sum_{i=1}^{m-1}v_ix_i \in [(v_i)_{i\in \{1,\dots,m-1\}}] \]
	so gilt
		\begin{gather*}
		\omega(v_1,\dots,v_m)=\omega(v_1,\dots,v_{m-1},\sum_{i=1}^{m-1}v_ix_i)\\
		= \sum_{i=1}^{m-1}\omega(v_1,\dots,v_{m-1},v_i)x_i (=0)
		\end{gather*}
	womit (iii) bewiesen ist.
	\end{Lemma}
\paragraph{Bemerkung}
	(i) liefert eine äquivalente Formulierung von "`alternierend"' für $ m $-Linearformen, wenn $ \Char (K)\neq 2 $.
	Nämlich: sind $ v_1,\dots,v_m\in V $ mit $ v_i=v_j $ für $ i\neq j $, so gilt
		\begin{align*}
		0 &= \omega(\dots,v_i\dots,v_j,\dots)+\omega(\dots,v_j,\dots,v_i,\dots)\\
		&= 2\omega(\dots,v_i,\dots,v_j,\dots) \Rightarrow 0 = \omega(\dots,v_i,\dots,v_j,\dots)
		\end{align*}
\paragraph{Buchhaltung}
	Benutzt man (vgl. Gausssches Eliminationsverfahren) die Elementarmatrizen
	\begin{gather*}
		D_i = (d_{kl}) \in Gl(m); d_{kl} = \delta_{kl}+(d-1)\delta_{ik}\delta_{il}\quad (d\in K^\times);\\
		T_{ij} = (t_{kl}) \in Gl(m); t_{kl} = \delta_{kl}-(\delta_{ik}-\delta_{jk})(\delta_{il}-\delta_{jl});\\
		S_{ij} = (s_{kl})\in Gl(m); s_{kl}=\delta_{kl}+s\delta_{ik}\delta_{jl}\quad (s\in K)
	\end{gather*}
	und beschreibt man eine Familie $ (v_i)_{i\in \{1,\dots,m\}} $ von Vektoren $ v_i\in V $ durch ein $ m $-Tupel $ A=(v_1,\dots,v_m) $ von Werten der Familie, so lassen sich die Homogenität und Eigenschaften (i) und (ii) des Lemmas einfach schreiben als
		\[ \omega(AD_i) = \omega(A)d,\ \omega(AT_{ij}) = -\omega(A),\ \omega(AS_{ij}(s)) = \omega(A) \]
\subsection{Wiederholung \& Definition}
	Die bijektiven Abbildungen (Permutationen)
		\[ \sigma: I\to I, i\mapsto \sigma(i),\text{ der Menge }I = \{1,\dots,m\} \]
	bilden eine Gruppe (mit der Komposition als Verknüpfung), die Permutationsgruppe $ S_m $ der Menge $ I $.
	\begin{Definition}[Transposition]
	Eine Transposition $ \tau_{ij} \in S_m, i\neq j $ ist eine Permutation, die zwei Indizes vertauscht,
		\[ \tau_{ij}:I\to I, k\mapsto \tau_{ij}(k):=
		\begin{cases}
			j, &\text{falls } k=i,\\
			i, &\text{falls } k=j,\\
			k & \text{sonst}.
		\end{cases} \]
	Jede Permutation ist eine Komposition von Transpositionen, wie man leicht durch Induktion über m zeigt:
	
	Ist $ \sigma(m) = i<m$, so ist $ \tau_{im}\circ \sigma $ eine Permutation, die $ m $ fixiert, also
		\[ \tau_{im}\circ\sigma\mid_{\{1,\dots,m-1\}}\in S_{m-1} \]
	\end{Definition}
\paragraph{Bemerkung}
	Die Eigenschaft (i) des Lemmas, $ \omega(AT_{ij})=-\omega(A) $, lässt sich mit $ \tau_{ij} $ dann formulieren als
		\[ \omega(v_{\tau_{ij}(1)}, \dots, v_{\tau_{ij}(m)}) = - \omega(v_1,\dots,v_m) \]
	Da jede Permutation $ \sigma\in S_m $ Komposition von Transpositionen ist, folgt
		\[ \forall \sigma\in S_m: \omega(v_{\sigma}(1),\dots,v_{\sigma(m)})=\pm \omega(v_1,\dots,v_{m}). \]
	Frage: Was ist das Vorzeichen bzw. wie kann man es berechnen?
\subsection{Lemma \& Definition}
	\begin{Definition}[Signum einer Permutation]
	Das Signum einer Permutation $ \sigma\in S_m $ ist die Zahl
		\[ \operatorname{sgn}\sigma := \prod_{i<j} \frac{\sigma(i)-\sigma(j)}{i-j}\in \{\pm 1\}; \]
	ist $ \sgn\sigma = 1 $, so heißt $ \sigma $ gerade, sonst ungerade. Signum liefert einen Gruppenhomomorphismus
		\[ \sgn: S_m\to (\{\pm 1\},\cdot). \]
	\end{Definition}
\paragraph{Beispiel}
	Eine Transposition $ \tau_{ij} $ ist eine ungerade Permutation, da
		\[ \sgn\tau_{ij} = \prod_{k<l}\frac{\tau_{ij}(k)-\tau_{ij}(l)}{k-l} = \frac{j-i}{i-j}\prod_{k\neq i,j}\frac{i-k}{j-k}\frac{j-k}{i-k} = -1 \]
\paragraph{Beweis}
	Seien $ \sigma,\tau\in S_m $ beliebig, dann gilt
	\[ \sgn(\tau\circ\sigma) = \prod_{i<j}\frac{\tau(\sigma(i))-\tau(\sigma(j))}{\sigma(i)-\sigma(j)}\frac{\sigma(i)-\sigma(j)}{i-j} = \prod_{i<j}\frac{\tau(\sigma(i))-\tau(\sigma(j))}{\sigma(i)-\sigma(j)}\prod_{i<j}\frac{\sigma(i)-\sigma(j)}{i-j} \]
	mit $ i'=\sigma(i), j'=\sigma(j) $ folgt
	\[ \prod_{i'<j'}\frac{\tau(i')-\tau(j')}{i'-j'}\prod_{i<j}\frac{\sigma(i)-\sigma(j)}{i-j} = \sgn(\tau)\cdot \sgn(\sigma). \]
	Da jede Permutation Komposition von Transpositionen ist, folgt daraus
		\[ \forall \sigma \in S_m:\sgn(\sigma) = \pm 1 \]
	und dass
		\[ \sgn:S_m \to (\{\pm 1\},\cdot) \]
	Gruppenhomomorphismus ist.
\paragraph{Bemerkung}
	Damit folgt für $ \omega\in \Lambda^mV^* $ und $ \sigma \in S_m $
		\[ \omega(v_{\sigma(1)},\dots,v_{\sigma(m)}) =\omega(v_1,\dots,v_m)\sgn\sigma. \]
\subsection{Leibniz-Formel}
	\begin{Satz}[Leibniz-Formel]
		Seien $ \omega\in \Lambda^mV^* $, $ (b_i)_{i\in \{1,\dots,m\}} $ linear unabhängig und $ (v_j)_{j\in \{1,\dots,m\}} $ eine Familie in der linearen Hülle der Familie $ [(b_i)_{i\in\{1,\dots,m\}}] \subset V$,
		\[ \forall j=1,\dots,m: v_j = \sum_{i=1}^{m}b_ix_{ij} \]
	dann gilt
		\[ \omega(v_1,\dots,v_m)=\omega(b_1,\dots,b_m)\sum_{\sigma\in S_m}\sgn(\sigma)x_{\sigma(1)1} \cdots x_{\sigma(m)m} \]
	\end{Satz}
\paragraph{Beweis}
	Ausmultiplizieren ergibt:
		\[ \omega(v_1,\dots,v_m)=\sum_{i_1=1}^{m}\cdots \sum_{i_m=1}^{m}\omega(b_{i_1},\dots,b_{i_m})x_{i_11}\cdots x_{i_mm} \]
	wegen $ \omega(\dots)=0 $, wenn zwei $ b $'s gleich sind, d.h. wann immer $ \{1,\dots,m \}\ni j\mapsto i_j \in \{1,\dots,m\} $ nicht injektiv ist, also keine Permutation ist.
		\[ = \sum_{\sigma\in S_m}\omega(b_{\sigma(1)},\dots,b_{\sigma(m)})x_{\sigma(1)1}\cdots x_{\sigma(m)m} = \sum_{\sigma\in S_m}\omega(b_1,\dots,b_m)\sgn(\sigma)x_{\sigma(1)1}\cdots x_{\sigma(m)m}\]
\paragraph{Beispiel}
	Ist die Koeffizientenmatrix $ x=(x_{ij})_{i,j\in\{1,\dots,m\}} $ in der Leibniz-Formel eine obere Dreiecksmatrix, d.h.
		\[ X= \begin{pmatrix}
		x_{11} & \cdots & \cdots & x_{1m}\\
		0 & x_{22} & &\vdots\\
		\vdots & \ddots & \ddots &\vdots\\
		0 & \cdots & 0 & x_{mm}
		\end{pmatrix}
		\text{ und } \forall j=1,\dots,m :v_j=\sum_{i=1}^{j}b_ix_{ij} \]
	so gilt für jede Permutation $ \sigma\in S_m $ von $ I=\{1,\dots,m \} $
		\[ x_{\sigma(1)1},\dots,x_{\sigma(m)m}\neq 0 \Rightarrow \forall j\in I:\sigma(j)\leq j \Rightarrow \sigma = \id_I \]
	und damit 
		\[ \omega(v_1,\dots,v_m) = \omega(b_1,\dots,b_m) x_{11}\cdots x_{mm}. \]
\subsection{Buchhaltung}
	\begin{Definition}[Determinante]
		Mit 
		\[ A:= (v_1,\dots,v_m)=(b_1,\dots,b_m)X = BX \]
	und der Determinante
		\[ \det X := \sum_{\sigma\in S_m}\sgn(\sigma)x_{\sigma(1)1}\cdots x_{\sigma(m)m} \]
	der Koeffizientenmatrix $ X\in K^{m\times m} $ lässt sich die Leibniz-Formel auch kürzer schreiben als
		\[ \omega(A) = \omega(B)\det X. \]
	\end{Definition}
	Für $ m=2 $ und $ m=3 $ lässt sich $ \det X $ einfach berechnen:
		\begin{itemize}
			\item für $ m=2 $ ist
				\[ \det \begin{pmatrix}
						x_{11}& x_{12}\\ x_{21} & x_{22}
					\end{pmatrix}
					= x_{11}x_{22}-x_{21}x_{12}
				\]
			\item für $ m=3 $ mit Hilfe der Regel von Sarrus (zuerst zyklische (gerade) Permutationen, dann mit einem Fixpunkt, also Transpositionen)
				\[ \det \begin{pmatrix}
					x_{11} & x_{12} & x_{13}\\
					x_{21} & x_{22} & x_{23}\\
					x_{31} & x_{32} & x_{33}
				\end{pmatrix}
				 = 	 \begin{matrix}
				 x_{11}x_{22}x_{33}
				 +x_{21}x_{32}x_{13}
				 +x_{31}x_{12}x_{23}\\
				 -x_{11}x_{23}x_{32}
				 -x_{21}x_{33}x_{12}
				 -x_{31}x_{22}x_{13}
				 \end{matrix} \]
			\end{itemize}
	Für $ m>3 $ liefert der Laplacesche Entwicklungssatz eine Methode, die Terme (Permutationen) zu sortieren: Für fest gewähltes $ j\in \{1,\dots, m \} $ gilt
		\[ \det X = \sum_{i=1}^{m}(-1)^{i+j}x_{ij}\det X_{ij} \]
	mit
		\[ X_{ij} = (x_{kl})_{k\neq i, l\neq j} = 
		\begin{pmatrix}
			x_{11} & \cdots & x_{1(j-1)} & x_{1j} & x_{1(j+1)} & \cdots & x_{1m} \\ 
			\vdots & \ddots &  	&  	&  	&  	&  \\ 
			x_{(i-1)1} 	&  	&  	&  	&  	&  	& x_{(i-1)m} \\ 
			x_{i1} 	&	&  	& x_{ij} &  &  	& x_{im} \\ 
			x_{(i+1)1} &  &  &  &  &  & x_{(i+1)m} \\ 
			\vdots &  &  &  &  &  &  \\ 
			x_{m1} &  &  & x_{mj} &  &  & x_{mm}
			\end{pmatrix}  \]
	Nämlich: Ist o.B.d.A. $ v_m=b_i $ in der Leibniz-Formel, also $ x_{km}=\delta_{ik} $, so erhält man
		\[ \det X = \sum_{\sigma\in S_m}\sgn(\sigma)\prod_{j=1}^{m}x_{\sigma(j)j}=\sum_{\sigma\in S_m}\sgn(\sigma)\left(\prod_{j=1}^{m-1}x_{\sigma(j)j}\right)x_{\sigma(m)m} \]
	wobei $ x_{\sigma(m)m} = 0 $ für $ \sigma(m)\neq i $ und $ =1 $ für $ \sigma(m) = i $,
		\[ = \sum_{\sigma\in S_m, \sigma(m)=i}\sgn(\sigma)\prod_{j=1}^{m-1}x_{\sigma(j)j} = \sum_{\sigma\in S_{m-1}}(-1)^{m-i}\sgn(\sigma')\prod_{j=1}^{m-1}x_{\sigma'(j)j} = (-1)^{m-i}\det X_{im}. \]
	Im letzten Schritt werden die Transpositionen berücksichtigt die notwendig sind, um die nun "`gelöschte"' Zeile ans Ende zu verschieben. Ausmultiplizieren des o.B.d.A. $ m $-ten Eintrags in einer alternierenden $ m $-Form $ \omega\in \Lambda^mV^* $ liefert also 
		\[ \omega(v_1,\dots,v_m) = \sum_{i=1}^{m}\omega(v_1,\dots,v_{m-1},b_i)x_{im} = \omega(b_1,\dots,b_m)\sum_{i=1}^{m}(-1)^{m-1}x_{im}\det X_{im} \]
	und damit die Behauptung, da $ \omega(b_1,\dots,b_m)\neq 0 $ angenommen werden kann (siehe unten).
	Da wegen $ \sgn(\sigma^{-1})=(\sgn(\sigma))^{-1} = \sgn(\sigma) $
		\begin{gather*}
		\sum_{\sigma\in S_m}\sgn(\sigma)\prod_{j=1}^{m}x_{j\sigma(j)} =
		\sum_{\sigma\in S_m} \sgn(\sigma^{-1})\prod_{j=1}^{m}x_{\sigma^{-1}(j)j} =\\
		\sum_{\sigma^{-1}\in S_m} \sgn(\sigma^{-1})\prod_{j=1}^{m}x_{\sigma(j)j} =
		\sum_{\sigma\in S_m} \sgn(\sigma^{-1})\prod_{j=1}^{m}x_{\sigma(j)j}
		\end{gather*}
	\begin{Definition}[Transponierte Matrix]
		gleicht die Determinante einer Matrix $ X $ der ihrer Transponierten:
		\[ \det X^t = \det X \text{ mit } X^t := (x_{ji})_{i,j\in \{i,\dots, m \}} \]
	\end{Definition}
	Damit gilt der Laplacesche Entwicklungssatz auch für die Entwicklung nach einer Zeile von $ X $, anstelle nach einer Spalte, wie oben.
	
	Eine andere Möglichkeit zur Bestimmung von $ \det X $ liefert das Gausssche Eliminationsverfahren (hier mit elementaren Spaltenumformungen; es wird von rechts multipliziert), da (vgl. oben)
		\begin{align*}
		\det XD_i &= d\cdot \det X. (\Leftrightarrow \det D_iX^t = d\cdot \det X^t)\\
		\det XT_{ij} &= -\det X, (\Leftrightarrow\dots)\\
		\det XS_{ij} &= \det X.
		\end{align*}
\paragraph{Bemerkung}
	Diese "`Rechenmethoden"' sind von historischer Bedeutung, manchmal sind sie theoretisch praktisch, aber von beschränkter praktischer Bedeutung (seit man Computer hat).
\subsection{Beispiel \& Definition (Blockmatrix)}
	\begin{Definition}[Blockmatrix]
		Für eine Blockmatrix
			\[ X = \begin{pmatrix}
			X_{11} & X_{12}\\ 0 & X_{22}
			\end{pmatrix}\]
		mit
			\[ X_{11}\in K^{m\times m},\ X_{12}\in K^{m\times n},\ X_{22}\in K^{n\times n} \]
		gilt
			\[ \det X = \det X_{11} \cdot \det X_{22} \]
		Beweis in Übung.
	\end{Definition}
\subsection{Beispiel \& Definition (Vandermonde-Determinante)}
	\begin{Definition}[Vandermonde-Determinante]
		Für $ x_1,\dots, x_k\in K $ hat die Vandermonde-Matrix
			\[ X= (x_i^{k-j})i,j\in \{1,\dots, k \} =
				\begin{pmatrix}
					x_1^{k-1}&x_1^{k-2}&\cdots & x_1^0\\
					x_2^{k-1}&x_2^{k-2}&\cdots & x_2^0\\
					\vdots & \vdots & \ddots&\vdots \\
					x_k^{k-1}&x_k^{k-2}&\cdots & x_k^0
				\end{pmatrix} \in K^{k\times k}\]
		(Vandermonde-)Determinante:
			\[ \det X = \det (x_i^{k-j})_{i,j = 1,\dots, k} = \prod_{i<j}x_i-x_j \]
		Denn: Für $ k=2 $ gilt
			\[ \det \begin{pmatrix}
				x_1&1\\x_2&1
			\end{pmatrix} = x_1-x_2\]
		also ist die Induktionsvoraussetzung wahr. Für $ k>2 $ gilt
			\[ XS_{21}(-x_k)\cdots S_{k(k-1)}(-x_k) = 
			\begin{pmatrix}
				x_1^{k-1} &\cdots & x_1 & 1\\
				\vdots & & \vdots & \vdots\\
				x_k^{k-1} & \cdots & x_k & 1
			\end{pmatrix}\begin{pmatrix}
				1 & 0 & \cdots & \\
				-x_k & 1 & 0 & \cdots\\
				0 & \ddots & \ddots & \\
				\vdots & & & 1
			\end{pmatrix}\begin{pmatrix}
			1 & 0 & \cdots & \\
			0 & 1& \ddots1&\\
			\vdots & 0 &1 & 0 \\
			\cdots & & -x_k & 1			
			\end{pmatrix}\]
			\[ = \begin{pmatrix}
			x_1^{k-1}-x_1^{k-2}x_k & x_1^{k-2}-x_1^{k-3}x_k & \cdots & x_1-x_k & 1 \\ 
			\vdots & \vdots &  & \vdots & \vdots \\ 
			x_{k-1}^{k-1}-x_{k-1}^{k-2}x_k & x_{k-1}^{k-2}-x_{k-1}^{k-3}x_k & \cdots & x_{k-1}-x_k & 1 \\ 
			0 & 0 & \cdots & 0 & 1
			\end{pmatrix}  \]
			\[ = \begin{pmatrix}
			(x_1-x_k)x_1^{k-2} & (x_1-x_k)x_1^{k-3} & \cdots & x_1-x_k & 1 \\ 
			\vdots & \vdots &  & \vdots & \vdots \\ 
			(x_{k-1}-x_k)x_{k-1}^{k-2}& (x_{k-1}-x_k)x_{k-1}^{k-3} & \cdots & x_{k-1}-x_k & 1 \\ 
			0 & 0 & \cdots & 0 & 1
			\end{pmatrix}  \]
		also ist (Laplacescher Entwicklungssatz nach letzter Zeile):
			\[ \det X = 0+(-1)^{k+k}\det 
				\begin{pmatrix}
					(x_1-x_k)x_1^{k-2} & (x_1-x_k)x_1^{k-3} & \cdots & x_1-x_k \\ 
					\vdots & \vdots &  & \vdots  \\ 
					(x_{k-1}-x_k)x_{k-1}^{k-2}& (x_{k-1}-x_k)x_{k-1}^{k-3} & \cdots & x_{k-1}-x_k  \\ 
				\end{pmatrix} \]
			\[ = 1\cdot(x_1-x_k)\cdots (x_{k-1}-x_k)\det
				\begin{pmatrix}
				x_1^{k-2} & \cdots & 1 \\ 
				\vdots &  & \vdots \\ 
				x_{k-1}^{k-2} & \cdots & 1
				\end{pmatrix} = \]
				\[ (x_1-x_k)\cdots(x_{k-1}-x_k)\prod_{i<j,\ i,j\in\{1,\dots, k-1 \}}(x_i-x_j) = \prod_{i<j,\ i,j\in \{1,\dots,k\}}(x_i-x_j) \]
		Also folgt die Behauptung mit Induktion.
	\end{Definition}
\subsection{Fortsetzungssatz für Determinantenformen}
	\begin{Satz}[Fortsetzungssatz für Determinantenformen]
		Ist $ (b_i)_{i\in \{1,\dots,n\}} $ eine Basis von $ V $ (also $ \dim V = n $) und $ d\in K $, so gilt:
		\[ \exists! \omega\in\Lambda^nV^*:\omega(b_1,\dots,b_n) = d \]
	\end{Satz}
\paragraph{Beweis}
	Eindeutigkeit folgt aus der Leibniz-Formel und der Tatsache, dass $ V=[(b_i)_{i\in\{1,\dots,n\}}] $.
	
	Existenz: Gegeben sind eine Basis $ (b_i)_{i\in \{1,\dots, n\}} $ von $ V $ und $ d\in K $. Wir definieren $ \omega $ durch die Leibniz-Formel:
		\[ \omega: V\times \overset{n}{\dots} \times V \to K, \omega(v_1,\dots,v_n):=d\cdot\det X \]
	wobei $ X\in K^{n\times n} $ die Koeffizientenmatrix für die Basisdarstellung der $ (v_i) $ ist,
		\[ (v_1,\dots,v_n)=(b_1,\dots,b_n)\cdot X. \]
	Dann gilt:
		\begin{itemize}
			\item $ \omega $ ist wohldefiniert, da $ (b_i)_{i\in \{i,\dots,n\}} $ linear unabhängig ist, womit die Koeffizienten $ x_{ij},\ i=1,\dots,n $ für jedes $ j=1,\dots,n $ eindeutig sind.
			\item $ \omega $ ist $ n $-Form, d.h.
				\[ \forall j=1,\dots,n: v_j\mapsto d\cdot\det X\]
			ist linear; offensichtlich!
			\item $ \omega $ ist alternierend, d.h.
				\[ \omega(v_1,\dots,v_n)=0\text{ falls } v_i = v_j \text{ für }i\neq j; \]
			ist aber $ v_i = v_j $ für ein Paar $ (i,j) $ mit $ i\neq j $, so gilt:
				\[ \sgn(\sigma)x_{\sigma(1)1}\cdots x_{\sigma(n)n} + \sgn(\tau_{ij}\circ \sigma) x_{(\tau_{ij}\circ \sigma)(1)1}\cdots x_{(\tau_{ij}\circ \sigma)(n)n} = 0,  \]
			denn
				\[ \sgn(\tau_{ij}\circ \sigma) = -\sgn(\sigma),\text{ und } x_{(\tau_{ij}\circ\sigma)(1)1}\cdots x_{(\tau_{ij}\circ\sigma)(n)n} = x_{\sigma(1)1}\cdots x_{\sigma(n)n} \]
			da $ v_i = v_j $
			
			Damit folgt:
				\[ \sum_{\sigma\in S_n}\sgn(\sigma)x_{\sigma(1)1}\cdots x_{\sigma(n)n}=0, \]
			da mit $ \sigma\in S_n $ auch $ \tau_{ij}\circ\sigma\in S_n $ in der Summe vorkommt.
		\end{itemize}	
\subsection{Korollar}
	\begin{Korollar}[Dimension der Determinantenform]
		Ist $ \dim V = n $, so ist $ \dim\Lambda^nV^* = 1 $. Beweis in der Übung.
	\end{Korollar}
\subsection{Korollar}
	Ist $ \omega(v_1,\dots,v_n)=0 $ für eine Determinantenform $ \omega\in\Lambda^nV^*\setminus\{0\}, $ so ist die Familie $ (v_i)_{i\in \{1,\dots, n\}} $ linear abhängig.
\paragraph{Beweis}
	Ist $ \omega\neq 0 $, so existiert eine Basis $ (b_i)_{i=1,\dots,n} $ von $ V $ mit $ \omega(b_1,\dots,b_n)=d\neq 0 $. Mit
		\[ (v_1,\dots,v_n)=(b_1,\dots,b_n)\cdot X \]
	folgt nun (Leibniz-Formel)
		\[ \omega(v_1,\dots,v_n)=\omega(b_1,\dots,b_n)\det X = d\cdot \det X \]
	Annahme: $ (v_i)_{i\in \{1,\dots,n\}} $ ist linear unabhängig, d.h. $ (v_i) $ ist Basis und damit existiert $ Y\in K^{n\times n} $ mit
		\[ (b_1,\dots,b_n)=(v_1,\dots,v_n)Y, \]
	nach Leibniz-Formel ist damit
		\[ 0\neq d = \omega(b_1,\dots,b_n)=\omega(v_1,\dots,v_n)\cdot \det Y. \]
	Damit folgt
		\[ \omega(v_1,\dots,v_n)\neq 0 \text{ (und }\det Y \neq 0) \]
\paragraph{Verkürzung des Beweises}
	Der Beweis ist zu verkürzen, indem man für die linear unabhängige Familie gleich die Basis $ (b_i) $ verwendet.
\paragraph{Bemerkung}
	Ist also $ \dim V=n $ und sind $ \omega\in \Lambda^nV^* $ und $ (b_i)_{i\in \{1,\dots,n\}} $ eine Familie in $ V $, so folgt aus zwei der folgenden Aussagen die dritte:
		\begin{enumerate}[(i)]
			\item $ \omega\neq 0 $
			\item $ \omega(b_1,\dots,b_n)\neq 0 $
			\item $ (b_i)_{i\in\{1,\dots,n\}} $ ist Basis von $ V $.
		\end{enumerate}
\paragraph{Bemerkung}
	Weiter folgt damit: Sind $ f\in \End(V),\ (b_i)_{i\in \{1,\dots,n\}} $ Basis von $ V $ und $ \omega\in \Lambda^nV^*\setminus\{0\}, $ so gilt:
		\[ f\in Gl(V)\Leftrightarrow \omega(f(b_1),\dots,f(b_n))\neq 0 \]
	bzw.
		\[ Gl(V)=\{f\in \End(V)\mid \omega(f(b_1),\dots,f(b_n))\neq 0\} \]
