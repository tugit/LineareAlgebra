%VO21-2016-01-07
\chapter{Volumenmessung}
Grundlegende Idee: Wir definieren ein Spat- oder Parallelotop-Volumen.

Algebraisch: Dieses Volumen kann dann benutzt werden, um zu testen, wann ein Spat/Pa"-ral"-lelotop "`zusammenklappt"'.
\section{Determinantenformen}
 Idee: Für den Flächeninhalt $ F(v,w) $ eines von zwei Vektoren $ v,w\in V $ aufgespannten Parallelogramms gilt
 \begin{align*}
 	F{(vx,w)} & = F{(v,w)}\cdot x \\
 	F(v+v',w) & = F(v,w)+F(v',w)
 \end{align*}
 und entsprechend für das zweite Argument.

 \definecolor{ttttff}{rgb}{0.2,0.2,1}
 \definecolor{ttfftt}{rgb}{0.2,1,0.2}
 \definecolor{uququq}{rgb}{0.25,0.25,0.25}
 \definecolor{qqqqff}{rgb}{0,0,1}

 %-------------------Begin Addition mit einem Vektor ----------------
 \begin{figure}[H]\centering
 	\tdplotsetmaincoords{0}{0} %-27
 	\begin{tikzpicture}[yscale=1,tdplot_main_coords]

 		\def\xstart{0} %x Koordinate der Startposition der Grafik
 		\def\ystart{0} %y Koordinate der Startposition der Grafik
 		\def\myscale{0.9} %ändert die Größe der Grafik (Skalierung der Grafik)

 		\def\xstartdraw{(\xstart + 1.5)} %xKoordinate des Referenzstartpunktes (in dieser Zeichnung: a)
 		\def\ystartdraw{(\ystart + 3.5)}%yKoordinate des Referenzstartpunktes (in dieser Zeichnung: a)

 		\def\balkenhoehe{(5.3)}% Länge des vertikalen blauen Balkens
 		\def\balkenlaenge{(10)}% Länge des horizontalen blauen Balkens
 		\def\balkenbreite{0.4} %Balkenbreite

 		%---------Begin Balken----------
 		\def\drehwinkel{0}
 		\node (VekV) at ({\xstart+0.7*cos(\drehwinkel)-\balkenbreite*sin(\drehwinkel)},{\ystart+0.5*sin(\drehwinkel)+\balkenbreite*cos(\drehwinkel)})[right, xshift=1,color=blue] {$V$};
 		\node (AffA) at ({\xstart+(\balkenlaenge-1)*cos(\drehwinkel)},{\ystart+(\balkenlaenge-1)*sin(\drehwinkel)+\balkenbreite*cos(\drehwinkel)})[color=red] {$A^2$};

 		\path[ shade, top color=white, bottom color=blue, opacity=.6]
 		({\xstart},{\ystart},0)  -- ({\xstart - \balkenbreite * cos(\drehwinkel)- (-\balkenbreite+0)*sin(\drehwinkel)},{\ystart - \balkenbreite * sin(\drehwinkel)+ (-\balkenbreite+0)*cos(\drehwinkel)},0)  -- ({\xstart - \balkenbreite * cos(\drehwinkel)- (\balkenhoehe+0.5)*sin(\drehwinkel)},{\ystart - \balkenbreite * sin(\drehwinkel)+ (\balkenhoehe+0.5)*cos(\drehwinkel)},0) -- ({\xstart - 0 * cos(\drehwinkel)- (\balkenhoehe+0)*sin(\drehwinkel)},{\ystart - 0 * sin(\drehwinkel)+ (\balkenhoehe+0)*cos(\drehwinkel)},0) -- cycle;

 		\path[ shade, right color=white, left color=blue, opacity=.6]
 		({\xstart},{\ystart},0)  -- ({\xstart - \balkenbreite * cos(\drehwinkel)- (-\balkenbreite+0)*sin(\drehwinkel)},{\ystart - \balkenbreite * sin(\drehwinkel)+ (-\balkenbreite+0)*cos(\drehwinkel)},0) --
 		({\xstart + (\balkenlaenge+0.5) * cos(\drehwinkel)- (-\balkenbreite+0)*sin(\drehwinkel)},{\ystart + (\balkenlaenge+0.5) * sin(\drehwinkel)+ (-\balkenbreite+0)*cos(\drehwinkel)},0) --
 		({\xstart + \balkenlaenge * cos(\drehwinkel)},{\ystart + \balkenlaenge * sin(\drehwinkel)},0)--
 		cycle;
 		%---------End Balken----------
 		\def\lightoffset{0.2*\myscale} %offeset der Vektoren

 		%Punkte Definition
 		\node (pointa1) at ({\xstartdraw},{\ystartdraw}) {};
 		\node (pointa2) at ({\xstartdraw+(1 *\myscale)},{\ystartdraw-(2.0*\myscale)}) {};
 		\node (pointb1) at ($(pointa1) + (3.0*\myscale,-1.0*\myscale) $) {};
 		\node (pointb2) at ($(pointb1) + (1.0*\myscale,-2.0*\myscale) $) {};

 		\node (pointc1) at ($(pointa1) + (6.5*\myscale,1.3*\myscale) $) {};
 		\node (pointc2) at ($(pointa2) + (6.5*\myscale,1.3*\myscale) $) {};

 		\node (pointFvwi) at ($(pointb2) + (-0.9*\myscale,0.9*\myscale) $) {};
 		\node (pointFvwa) at ($(pointb2) + (-1.5*\myscale,-0.3*\myscale) $) {};

 		\node (pointFvswi) at ($(pointc2) + (-2.9*\myscale,-1.2*\myscale) $) {};
 		\node (pointFvswa) at ($(pointc2) + (-1.3*\myscale,-1.9*\myscale) $) {};

 		\node (pointfgi) at ($(pointa1) + (2.2*\myscale,-0.2*\myscale) $) {};
 		\node (pointfga) at ($(pointfgi) + (-1.4*\myscale,1.8*\myscale) $) {};



 		%Flächen füllen
 		%blaue Flaeche
 		\fill[color=ttttff,fill=ttttff,fill opacity=0.15] (pointa1.center) -- (pointb1.center) -- (pointc1.center) -- (pointc2.center) -- (pointb2.center)-- (pointa2.center)-- cycle;
 		%gruene Flaeche
 		\fill[color=ttfftt,fill=ttfftt,fill opacity=0.5] (pointa1.center) -- (pointc1.center) -- (pointc2.center) -- (pointa2.center)-- cycle;

 		%Vektoren blau
 		\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointa1) -- (pointb1);
 		\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointa2) -- (pointb2);
 		\node [color=blue] (pointlabelg1) at ($(pointa1)!0.5!(pointb1)$) [above, xshift=0, yshift=0] {$v$} ;
 		\node [color=blue] (pointlabelg2) at ($(pointa2)!0.5!(pointb2)$) [above, xshift=0, yshift=0] {$v$} ;

 		\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointb1) -- (pointc1);
 		\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointb2) -- (pointc2);
 		\node [color=blue] (pointlabelg3) at ($(pointb1)!0.5!(pointc1)$) [above, xshift=0, yshift=0] {$v'$} ;
 		\node [color=blue] (pointlabelg4) at ($(pointb2)!0.5!(pointc2)$) [above, xshift=0, yshift=0] {$v'$} ;

 		\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointa2) -- (pointa1);
 		\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointb2) -- (pointb1);
 		\draw[-{>[scale=1,length=10,width=6]},shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointc2) -- (pointc1);

 		\node [color=blue] (pointlabelga2a1) at ($(pointa2)!0.5!(pointa1)$) [left, xshift=0, yshift=0] {$w$} ;
 		\node [color=blue] (pointlabelgb2b1) at ($(pointb2)!0.5!(pointb1)$) [right, xshift=0, yshift=0] {$w$} ;
 		\node [color=blue] (pointlabelgc2c1) at ($(pointc2)!0.5!(pointc1)$) [right, xshift=0, yshift=0] {$w$} ;

 		%Vektoren gruen
 		\draw[-{>[scale=1,length=10,width=6]},shorten >=4pt, shorten <=4pt,line width=0.2pt,color=green] (pointa1) -- (pointc1);
 		\draw[-{>[scale=1,length=10,width=6]},shorten >=4pt, shorten <=4pt,line width=0.2pt,color=green] (pointa2) -- (pointc2);
 		\node [color=green] (pointlabelac1) at ($(pointa1)!0.5!(pointc1)$) [above, xshift=0, yshift=0] {$v+v'$} ;
 		\node [color=green] (pointlabelac2) at ($(pointa2)!0.5!(pointc2)$) [below, xshift=10, yshift=5] {$v+v'$} ;

 		%Punkte malen
 		\draw[fill,color=red] (pointa1) circle [x=1cm,y=1cm,radius=0.08]node[above, xshift=0, yshift=0]{};
 		\draw[fill,color=red] (pointb1) circle [x=1cm,y=1cm,radius=0.08]node[above, xshift=0, yshift=0]{};
 		\draw[fill,color=red] (pointa2) circle [x=1cm,y=1cm,radius=0.08]node[below, xshift=5, yshift=0]{};
 		\draw[fill,color=red] (pointb2) circle [x=1cm,y=1cm,radius=0.08]node[below, xshift=5, yshift=0]{};
 		\draw[fill,color=red] (pointc1) circle [x=1cm,y=1cm,radius=0.08]node[below, xshift=5, yshift=0]{};
 		\draw[fill,color=red] (pointc2) circle [x=1cm,y=1cm,radius=0.08]node[below, xshift=5, yshift=0]{};

 		\draw[->,shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointFvwa) -- (pointFvwi);
 		\draw[->,shorten >=2pt, shorten <=2pt,line width=0.2pt,color=blue] (pointFvswa) -- (pointFvswi);
 		\draw[->,shorten >=2pt, shorten <=2pt,line width=0.2pt,color=green] (pointfga) -- (pointfgi);

 		\node [color=blue] (pointlabelFvwl) at (pointFvwa) [xshift=0.5, yshift=-0.5] {$F(v,w)$} ;
 		\node [color=blue] (pointlabelFvswl) at (pointFvswa) [xshift=-0.5,  yshift=-5] {$F(v',w)$} ;
 		\node [color=green] (pointlabelFvswl) at (pointfga) [xshift=35 ] {$F(v+v',w)=\textcolor{blue}{F(v,w)+F(v',w)}$} ;

 	\end{tikzpicture}
 \end{figure}
 %-------------------End Addition mit einem Vektor ----------------


 Außerdem verschwindet der Flächeninhalt, wenn das Parallelogramm "`zusammenklappt"', also insbesondere gilt
 \[
 	w=v\Rightarrow F(v,w)=0
 \]
 Die folgende Definition verallgemeinert diese Eigenschaften:

 \subsection{Definition}
 	\begin{Definition}[Linearform/Determinantenform]
 		Sei $ V $ ein $ K $-VR. Eine Abbildung $ \omega:V^m\to K $ heißt
 		\begin{itemize}
 			\item \emph{$ m $-linear}, bzw. eine \emph{$ m $-(Linear-)Form}, falls $ \omega $ in jedem Argument linear ist, d.h.
 			      \[
 			      	\forall i=1,\dots, m: V\ni v_i\mapsto \omega(v_1,\dots,v_{i-1},v_i,v_{i+1},\dots,v_m)\in K
 			      \]
 			      ist linear;
 			\item \emph{alternierend}, falls $ \omega(v_1,\dots,v_m)=0 $ wann immer zwei Vektoren gleich sind, d.h.
 			      \[
 			      	v_i = v_j \text{ für } i\neq j \Rightarrow \omega(v_1,\dots, v_m) = 0.
 			      \]
 		\end{itemize}
 		Die Menge der alternierenden $ m $-Formen wird mit $ \Lambda^mV^* $ bezeichnet. Ist $ \dim V = n $, so heißt ein $ \omega\in \Lambda^nV^* $ auch \emph{Determinantenform}.
 	\end{Definition}

 	\paragraph{Beispiel}
 		Jede Linearform $ \omega\in V^* $ ist eine (alternierende) 1-Form, $ \Lambda^1V^*=V^* $.
 	\paragraph{Bemerkung}
 		$ \Lambda^mV^* $ ist für jedes $ m\in \mathbb{N} $ selbst ein $ K $-VR.
 \subsection{Lemma}
 	\begin{Lemma}
 		Für eine alternierende $ m $-Form $ \omega \in \Lambda^mV^* $ und $ i\neq j $ gilt:
 		\begin{enumerate}[(i)]
 			\item $ \omega(\dots,v_i,\dots,v_j,\dots) = -\omega (\dots,v_j,\dots,v_i,\dots)$;
 			\item $ \omega(\dots,v_i,\dots,v_is+v_j,\dots) = \omega(\dots,v_i,\dots,v_j,\dots) $ für $ s\in K $ \footnote{Geometrisch entspricht dies einer Scherung!};
 			\item $ \omega(v_1,\dots,v_m)=0 $, falls $ (v_i)_{i\in \{1,\dots,m\}} $ linear abhängig ist.
 		\end{enumerate}
 		\paragraph{Beweis}
 			Seien $ v_1,\dots,v_m\in V $ und $ i,j\in \{1,\dots,m\} $ mit $ i\neq j $. Dann gilt:
 			\begin{align*}
 				0 & = \omega(\dots,v_i+v_j,\dots,v_i+v_j,\dots)                                                                                               \\
 				  & = \omega(\dots,v_i,\dots,v_i,\dots)+\omega(\dots,v_j,\dots,v_j,\dots)\\& +\omega(\dots,v_i,\dots,v_j,\dots)+\omega(\dots,v_j,\dots,v_i,\dots) \\
 				  & = \omega(\dots,v_i,\dots,v_j,\dots)+\omega(\dots,v_j,\dots,v_i,\dots)
 				\intertext{und}
 				0 & =\omega(\dots,v_i,\dots,v_is,\dots)                                                                                                       \\
 				  & = \omega(\dots,v_i,\dots,v_is+v_j-v_j,\dots)                                                                                              \\
 				  & = \omega(\dots,v_i,\dots,v_is+v_j,\dots)-\omega(\dots,v_i,\dots,v_j,\dots)
 			\end{align*}
 			Dies beweist (i) und (ii).

 			Ist die Familie $ (v_i)_{i\in \{1,\dots,m\}} $ linear abhängig, o.B.d.A
 			\[
 				v_m = \sum_{i=1}^{m-1}v_ix_i \in [(v_i)_{i\in \{1,\dots,m-1\}}]
 			\]
 			so gilt
 			\begin{align*}
 				\omega(v_1,\dots,v_m) & =\omega(v_1,\dots,v_{m-1},\sum_{i=1}^{m-1}v_ix_i)                       \\
 				                      & = \sum_{i=1}^{m-1}\underbrace{\omega(v_1,\dots,v_{m-1},v_i)}_{0}x_i = 0
 			\end{align*}
 			womit (iii) bewiesen ist.
 		\end{Lemma}
 	\paragraph{Bemerkung}
 		(i) liefert eine äquivalente Formulierung von "`alternierend"' für $ m $-Linearformen, wenn $ \Char (K)\neq 2 $.
 		Nämlich: sind $ v_1,\dots,v_m\in V $ mit $ v_i=v_j $ für $ i\neq j $, so gilt
 		\begin{align*}
 			0             & = \omega(\dots,v_i\dots,v_j,\dots)+\omega(\dots,v_j,\dots,v_i,\dots) \\
 			              & = 2\omega(\dots,v_i,\dots,v_j,\dots)                                 \\
 			\Rightarrow 0 & = \omega(\dots,v_i,\dots,v_j,\dots)
 		\end{align*}
 	\paragraph{Buchhaltung}
 		Benutzt man (vgl. Gausssches Eliminationsverfahren) die Elementarmatrizen
 		\begin{gather*}
 			D_i = (d_{kl}) \in \mathrm{Gl}(m);\ d_{kl} = \delta_{kl}+(d-1)\delta_{ik}\delta_{il}\quad (d\in K^\times);\\
 			T_{ij} = (t_{kl}) \in \mathrm{Gl}(m);\ t_{kl} = \delta_{kl}-(\delta_{ik}-\delta_{jk})(\delta_{il}-\delta_{jl});\\
 			S_{ij} = (s_{kl})\in \mathrm{Gl}(m);\ s_{kl}=\delta_{kl}+s\delta_{ik}\delta_{jl}\quad (s\in K)
 		\end{gather*}
 		und beschreibt man eine Familie $ (v_i)_{i\in \{1,\dots,m\}} $ von Vektoren $ v_i\in V $ durch ein \emph{$ m $-Tupel} $ A=(v_1,\dots,v_m) $ von Werten der Familie, so lassen sich die \emph{Homogenität} und Eigenschaften (i) und (ii) des Lemmas einfach schreiben als
 		\[
 			\omega(AD_i(d)) = \omega(A)d,\ \omega(AT_{ij}) = -\omega(A),\ \omega(AS_{ij}(s)) = \omega(A)
 		\]
 \subsection{Wiederholung \& Definition}
 	Die bijektiven Abbildungen (Permutationen)
 	\[
 		\sigma: I\to I,\ i\mapsto \sigma(i),\text{ der Menge }I = \{1,\dots,m\}
 	\]
 	bilden mit der Komposition eine Gruppe: die Permutationsgruppe $ S_m $ der Menge $ I $.
 	\begin{Definition}[Transposition]
 		Eine \emph{Transposition} $ \tau_{ij} \in S_m, i\neq j $ ist eine Permutation, die zwei Indizes vertauscht,
 		\[
 			\tau_{ij}:I\to I,\ k\mapsto \tau_{ij}(k):=
 			\begin{cases}
 				j, & \text{falls } k=i, \\
 				i, & \text{falls } k=j, \\
 				k  & \text{sonst}.
 			\end{cases}
 		\]
 		Jede Permutation ist eine Komposition von Transpositionen, wie man leicht durch Induktion über $m$ zeigt:

 		Ist $ \sigma(m) = i<m$, so ist $ \tau_{im}\circ \sigma $ eine Permutation, die $ m $ fixiert, also
 		\[
 			\tau_{im}\circ\sigma\mid_{\{1,\dots,m-1\}}\in S_{m-1}
 		\]
 	\end{Definition}
 	\paragraph{Bemerkung}
 		Die Eigenschaft (i) des Lemmas, $ \omega(AT_{ij})=-\omega(A) $, lässt sich mit $ \tau_{ij} $ dann formulieren als
 		\[
 			\omega(v_{\tau_{ij}(1)}, \dots, v_{\tau_{ij}(m)}) = - \omega(v_1,\dots,v_m)
 		\]
 		Da jede Permutation $ \sigma\in S_m $ Komposition von Transpositionen ist, folgt
 		\[
 			\forall \sigma\in S_m: \omega(v_{\sigma(1)},\dots,v_{\sigma(m)})=\pm \omega(v_1,\dots,v_{m}).
 		\]
 		Frage: Was ist das Vorzeichen bzw. wie kann man es berechnen?
 \subsection{Lemma \& Definition}
 	\begin{Definition}[Signum einer Permutation]
 		Das Signum einer Permutation $ \sigma\in S_m $ ist die Zahl
 		\[
 			\operatorname{sgn}\sigma := \prod_{i<j} \frac{\sigma(i)-\sigma(j)}{i-j}\in \{\pm 1\};
 		\]
 		ist $ \sgn\sigma = 1 $, so heißt $ \sigma $ gerade, sonst ungerade. Signum liefert einen Gruppenhomomorphismus
 		\[
 			\sgn: S_m\to (\{\pm 1\},\cdot).
 		\]
 	\end{Definition}
 	\paragraph{Beispiel}
 		Eine Transposition $ \tau_{ij} $ ist eine ungerade Permutation, da
 		\[
 			\sgn\tau_{ij} = \prod_{k<l}\frac{\tau_{ij}(k)-\tau_{ij}(l)}{k-l} = \frac{j-i}{i-j}\prod_{k\neq i,j}\frac{i-k}{j-k}\frac{j-k}{i-k} = -1
 		\]

%VO22-2016-01-12
 	\paragraph{Beweis}
 		Seien $ \sigma,\tau\in S_m $ beliebig, dann gilt
 		\begin{align*} \sgn(\tau\circ\sigma) &= \prod_{i<j}\frac{\tau(\sigma(i))-\tau(\sigma(j))}{\sigma(i)-\sigma(j)}\cdot\frac{\sigma(i)-\sigma(j)}{i-j} \\
 			  & = \prod_{i<j}\frac{\tau(\sigma(i))-\tau(\sigma(j))}{\sigma(i)-\sigma(j)}\prod_{i<j}\frac{\sigma(i)-\sigma(j)}{i-j} \\
 			\intertext{Setze $i':=\sigma(i), j':=\sigma(j) $}
 			  & =\prod_{i'<j'}\frac{\tau(i')-\tau(j')}{i'-j'}\prod_{i<j}\frac{\sigma(i)-\sigma(j)}{i-j}                            \\
 			  & = \sgn(\tau)\cdot \sgn(\sigma).
 		\end{align*}
 		Da jede Permutation Komposition von Transpositionen ist, folgt daraus
 		\[
 			\forall \sigma \in S_m:\sgn(\sigma) = \pm 1
 		\]
 		und dass
 		\[
 			\sgn:S_m \to (\{\pm 1\},\cdot)
 		\]
 		Gruppenhomomorphismus ist.
 	\paragraph{Bemerkung}
 		Damit folgt für $ \omega\in \Lambda^mV^* $ und $ \sigma \in S_m $
 		\[
 			\omega(v_{\sigma(1)},\dots,v_{\sigma(m)}) =\omega(v_1,\dots,v_m)\sgn\sigma.
 		\]
 \subsection{Leibniz-Formel}
 	\begin{Satz}[Leibniz-Formel]
 		Seien $ \omega\in \Lambda^mV^* $, $ (b_i)_{i\in \{1,\dots,m\}} $ lin. unabhängig und $ (v_j)_{j\in \{1,\dots,m\}} $ eine Familie in $ [(b_i)_{i\in\{1,\dots,m\}}] \subset V$,
 		\[
 			\forall j=1,\dots,m: v_j = \sum_{i=1}^{m}b_ix_{ij}
 		\]
 		dann gilt
 		\[
 			\omega(v_1,\dots,v_m)=\omega(b_1,\dots,b_m)\sum_{\sigma\in S_m}\sgn(\sigma)x_{\sigma(1)1} \cdots x_{\sigma(m)m}
 		\]
 	\end{Satz}
 	\paragraph{Beweis}
 		Ausmultiplizieren ergibt:
 		\begin{align*} \omega(v_1,\dots,v_m)&=\sum_{i_1=1}^{m}\cdots \sum_{i_m=1}^{m}\omega(b_{i_1},\dots,b_{i_m})x_{i_11}\cdots x_{i_mm}
 			\intertext{Es gilt: $ \omega(\dots)=0 $, wenn zwei $ b $'s gleich sind, d.h. wann immer $ \{1,\dots,m \}\ni j\mapsto i_j \in \{1,\dots,m\} $ nicht injektiv ist, also keine Permutation ist.}
 			  & = \sum_{\sigma\in S_m}\omega(b_{\sigma(1)},\dots,b_{\sigma(m)})x_{\sigma(1)1}\cdots x_{\sigma(m)m} \\
 			  & = \sum_{\sigma\in S_m}\omega(b_1,\dots,b_m)\sgn(\sigma)x_{\sigma(1)1}\cdots x_{\sigma(m)m}
 		\end{align*}
 	\paragraph{Beispiel}
 		Ist die \emph{Koeffizientenmatrix} $ x=(x_{ij})_{i,j\in\{1,\dots,m\}} $ in der Leibniz-Formel eine obere Dreiecksmatrix, d.h.
 		\[
 			X=
 			\begin{pmatrix}
 				x_{11} & \cdots & \cdots & x_{1m} \\
 				0      & x_{22} &        & \vdots \\
 				\vdots & \ddots & \ddots & \vdots \\
 				0      & \cdots & 0      & x_{mm}
 			\end{pmatrix}
 			\quad\text{und}\quad \forall j=1,\dots,m :v_j=\sum_{i=1}^{j}b_ix_{ij}
 		\]
 		so gilt für jede Permutation $ \sigma\in S_m $ von $ I=\{1,\dots,m \} $
 		\begin{align*} x_{\sigma(1)1},\dots,x_{\sigma(m)m}\neq 0 &\Rightarrow \forall j\in I:\sigma(j)\leq j\\
 			  & \Rightarrow \sigma = \id_I
 		\end{align*}
 		und damit
 		\[
 			\omega(v_1,\dots,v_m) = \omega(b_1,\dots,b_m) x_{11}\cdots x_{mm}.
 		\]

 \subsection{Buchhaltung}
 	\begin{Definition}[Determinante]
 		Mit
 		\[
 			A:= (v_1,\dots,v_m)=\underbrace{(b_1,\dots,b_m)}_{:=B}X = BX
 		\]
 		und der \emph{Determinante}
 		\[
 			\det X := \sum_{\sigma\in S_m}\sgn(\sigma)x_{\sigma(1)1}\cdots x_{\sigma(m)m}
 		\]
 		der Koeffizientenmatrix $ X\in K^{m\times m} $ lässt sich die Leibniz-Formel auch kürzer schreiben als
 		\[
 			\omega(A) = \omega(B)\cdot\det X.
 		\]
 	\end{Definition}
 	Für $ m=2 $ und $ m=3 $ lässt sich $ \det X $ einfach berechnen:
 	\begin{itemize}
 		\item für $ m=2 $ ist
 		      \[
 		      	\det
 		      	\begin{pmatrix}
 		      		x_{11} & x_{12} \\ x_{21} & x_{22}
 		      	\end{pmatrix}
 		      	= x_{11}x_{22}-x_{21}x_{12}
 		      \]
 		\item für $ m=3 $ mit Hilfe der \emph{Regel von Sarrus} (zuerst zyklische (gerade) Permutationen, dann mit einem Fixpunkt, also Transpositionen $\tau_{1,3}, \tau_{1,2}, \tau_{2,3}$)
 		      \[
 		      	\det
 		      	\begin{pmatrix}
 		      		x_{11} & x_{12} & x_{13} \\
 		      		x_{21} & x_{22} & x_{23} \\
 		      		x_{31} & x_{32} & x_{33}
 		      	\end{pmatrix}
 		      	=
 		      	\begin{matrix*}[r]
 		      		x_{11}x_{22}x_{33}
 		      		+x_{21}x_{32}x_{13}
 		      		+x_{31}x_{12}x_{23}\\
 		      		-x_{31}x_{22}x_{13}
 		      		-x_{21}x_{12}x_{33}
 		      		-x_{11}x_{32}x_{23}
 		      	\end{matrix*}
 		      \]
 		      \begin{center}
 		      	% source: http://www.texample.net/tikz/examples/mnemonic-rule-for-matrix-determinant/
 		      	\begin{tikzpicture}[baseline=(A.center)]
 		      		\tikzset{node style ge/.style={circle}}
 		      		\tikzset{BarreStyle/.style = {opacity=.4,line width=4 mm,line cap=round,color=#1}}
 		      		\tikzset{SignePlus/.style = {above left,,opacity=1,circle,fill=#1!50}}
 		      		\tikzset{SigneMoins/.style = {below left,,opacity=1,circle,fill=#1!50}}

 		      		\matrix (A) [matrix of math nodes, nodes = {node style ge},,column sep=0 mm]
 		      		{
                                        x_{11} & x_{12} & x_{13}  \\
 		      			x_{21} & x_{22} & x_{23}  \\
 		      			x_{31} & x_{32} & x_{33}  \\\hline
 		      			x_{11} & x_{12} & x_{13}  \\
 		      			x_{21} & x_{22} & x_{13}  \\
 		      		};

 		      		\draw [BarreStyle=blue] (A-1-1.north west) node[SignePlus=blue] {$+$} to (A-3-3.south east);
 		      		\draw [BarreStyle=blue] (A-2-1.north west) node[SignePlus=blue] {$+$} to (A-4-3.south east);
 		      		\draw [BarreStyle=blue] (A-3-1.north west) node[SignePlus=blue] {$+$} to (A-5-3.south east);
 		      		\draw [BarreStyle=red]  (A-3-1.south west) node[SigneMoins=red] {$-$} to (A-1-3.north east);
 		      		\draw [BarreStyle=red]  (A-4-1.south west) node[SigneMoins=red] {$-$} to (A-2-3.north east);
 		      		\draw [BarreStyle=red]  (A-5-1.south west) node[SigneMoins=red] {$-$} to (A-3-3.north east);
 		      	\end{tikzpicture}
 		      \end{center}
 	\end{itemize}
 	Für $ m>3 $ liefert der Laplacesche Entwicklungssatz eine Methode, die Terme (Permutationen) zu sortieren: Für fest gewähltes $ j\in \{1,\dots, m \} $ gilt\footnote{Entwicklung nach $j$-ter Spalte}
 	\[
 		\det X = \sum_{i=1}^{m}(-1)^{i+j}x_{ij}\det X_{ij}
 	\]
 	mit\footnote{Die $i$-te Zeile und die $j$-te Spalte sind in dieser Matrix "`gestrichen"', d.h. die Matrix ist aus $K^{(m-1)\times(m-1)}$}
 	\[
 		X_{ij} := (x_{kl})_{\substack{k\neq i\\ l\neq j}} =
 		% TODO: die i-te Zeile und j-te Spalte gehören durchgestrichen! % Q&D-Fix durch markierung in rot
 		\begin{pmatrix}
 			x_{11}     & \cdots & x_{1(j-1)} & \color{red}x_{1j} & x_{1(j+1)} & \cdots & x_{1m}     \\
 			\vdots     &        &            & \color{red}\vdots &            &        & \vdots     \\
 			x_{(i-1)1} &        &            & \color{red}\vdots &            &        & x_{(i-1)m} \\
 			\color{red}x_{i1} 	&\color{red} \dots	& \color{red}\dots & \color{red}x_{ij} &
 			\color{red}\dots &\color{red} \dots 	& \color{red}x_{im} \\
 			x_{(i+1)1} &        &            & \color{red}\vdots &            &        & x_{(i+1)m} \\
 			\vdots     &        &            & \color{red}\vdots &            &        & \vdots     \\
 			x_{m1}     &        &            & \color{red}x_{mj} &            &        & x_{mm}
 		\end{pmatrix}
 	\]
 	Nämlich: Ist o.B.d.A. $ v_m=b_i $ in der Leibniz-Formel, also $ x_{km}=\delta_{ik} $, so erhält man
 	\begin{align*}
 		\det X & = \sum_{\sigma\in S_m}\sgn(\sigma)\prod_{j=1}^{m}x_{\sigma(j)j}                          \\
 		       & =\sum_{\sigma\in S_m}\sgn(\sigma)\Big(\prod_{j=1}^{m-1}x_{\sigma(j)j}\Big)x_{\sigma(m)m}
 		\intertext{nach Voraussetzung gilt: $ x_{\sigma(m)m} = 0 $ für $ \sigma(m)\neq i $ und $x_{\sigma(m)m} =1 $ für $ \sigma(m) = i $,}
 		       & = \sum_{\substack{\sigma\in S_m                                                          \\ \sigma(m)=i}}\sgn(\sigma)\prod_{j=1}^{m-1}x_{\sigma(j)j} \\
 		       & = \sum_{\sigma'\in S_{m-1}}(-1)^{m-i}\sgn(\sigma')\prod_{j=1}^{m-1}x_{\sigma'(j)j}       \\
 		       & = (-1)^{m-i}\det X_{im}.
 	\end{align*}
 	Im vorletzten Schritt werden die Transpositionen berücksichtigt die notwendig sind, um die nun "`gestrichene"' $i$-te Zeile ans Ende zu verschieben.

 	Ausmultiplizieren des o.B.d.A. $ m $-ten Eintrags in einer alternierenden $ m $-Form $ \omega\in \Lambda^mV^* $ liefert also
 	\begin{align*}
 		\omega(v_1,\dots,v_m) & = \sum_{i=1}^{m}\omega(v_1,\dots,v_{m-1},b_i)x_{im}              \\
 		                      & = \omega(b_1,\dots,b_m)\sum_{i=1}^{m}(-1)^{m-1}x_{im}\det X_{im}
 	\end{align*}
 	und damit die Behauptung, da $ \omega(b_1,\dots,b_m)\neq 0 $ angenommen werden kann (siehe unten).

 	Da wegen $ \sgn(\sigma^{-1})=(\sgn(\sigma))^{-1} = \sgn(\sigma) $
 	\begin{align*}
 		\sum_{\sigma\in S_m}\sgn(\sigma)\prod_{j=1}^{m}x_{j\sigma(j)}
 		  & =\sum_{\sigma\in S_m} \sgn(\sigma^{-1})\prod_{j=1}^{m}x_{\sigma^{-1}(j)j} \\
 		  & =\sum_{\sigma^{-1}\in S_m} \sgn(\sigma^{-1})\prod_{j=1}^{m}x_{\sigma(j)j} \\
 		  & =\sum_{\sigma\in S_m} \sgn(\sigma)\prod_{j=1}^{m}x_{\sigma(j)j}
 	\end{align*}
 	\begin{Definition}[Transponierte Matrix]
 		gleicht die Determinante einer Matrix $ X $ der ihrer \emph{Transponierten}:
 		\[
 			\det X^t = \det X \quad\text{mit}\quad X^t := (x_{ji})_{i,j\in \{i,\dots, m \}}
 		\]
 	\end{Definition}
 	Damit gilt der Laplacesche Entwicklungssatz auch für die Entwicklung nach einer Zeile von $ X $, anstelle nach einer Spalte, wie oben.

 	Eine andere Möglichkeit zur Bestimmung von $ \det X $ liefert das Gausssche Eliminationsverfahren (hier mit elementaren Spaltenumformungen; es wird von rechts multipliziert), da (vgl. oben)
 	\begin{align*}
 		\det XD_i    & = d\cdot \det X \Leftrightarrow \det D_iX^t = d\cdot \det X^t \\
 		\det XT_{ij} & = -\det X  \Leftrightarrow T_{ij}X^t = -\det X^t              \\
 		\det XS_{ij} & = \det X  \Leftrightarrow S_{ij}X^t = \det X^t
 	\end{align*}
 	\paragraph{Bemerkung}
 		Diese "`Rechenmethoden"' sind von historischer Bedeutung, manchmal sind sie theoretisch praktisch, aber von beschränkter praktischer Bedeutung (seit man Computer hat).

%VO23-2016-01-14
 \subsection{Beispiel \& Definition (Blockmatrix)}
 	\begin{Definition}[Blockmatrix]
 		Für eine Blockmatrix
 		\[
 			X =
 			\begin{pmatrix}
 				X_{11} & X_{12} \\ 0 & X_{22}
 			\end{pmatrix}
 		\]
 		mit
 		\[
 			X_{11}\in K^{m\times m},\ X_{12}\in K^{m\times n},\ X_{22}\in K^{n\times n}
 		\]
 		gilt
 		\[
 			\det X = \det X_{11} \cdot \det X_{22}
 		\]
 		Beweis in Übung.
 	\end{Definition}
 \subsection{Beispiel \& Definition (Vandermonde-Determinante)}
 	\begin{Definition}[Vandermonde-Determinante]
 		Für $ x_1,\dots, x_k\in K $ hat die \emph{Vandermonde-Matrix}
 		\[
 			X= \big(x_i^{k-j}\big)_{i,j\in \{1,\dots, k \}} =
 			\begin{pmatrix}
 				x_1^{k-1} & x_1^{k-2} & \cdots & x_1^0  \\
 				x_2^{k-1} & x_2^{k-2} & \cdots & x_2^0  \\
 				\vdots    & \vdots    & \ddots & \vdots \\
 				x_k^{k-1} & x_k^{k-2} & \cdots & x_k^0
 			\end{pmatrix}
 			\in K^{k\times k}
 		\]
 		die \emph{(Vandermonde-)Determinante}:
 		\[
 			\det X = \det \big(x_i^{k-j}\big)_{i,j = 1,\dots, k} = \prod_{i<j}x_i-x_j
 		\]

 		Denn:

 		Für $ k=2 $ gilt
 		\[
 			\det
 			\begin{pmatrix}
 				x_1 & 1 \\x_2&1
 			\end{pmatrix}
 			= x_1-x_2
 		\]
 		also ist die Induktionsvoraussetzung gegeben.

 		Für $ k>2 $ gilt
 		\begin{align*}
 			&XS_{21}(-x_k)\cdots S_{k(k-1)}(-x_k) \\
 			&=
 			\begin{pmatrix}
 			x_1^{k-1} &\cdots & x_1 & 1\\
 			\vdots & & \vdots & \vdots\\
 			x_k^{k-1} & \cdots & x_k & 1
 			\end{pmatrix}
 			\begin{pmatrix}
 			1 & 0 &   & \\
 			-x_k & 1 &  & \\
 			&  & \ddots & \\
 			& & & 1
 			\end{pmatrix}
 			\dots
 			\begin{pmatrix}
 			1 &  &   & \\
 			& \ddots&  &\\
 			&   &1 & 0 \\
 			& & -x_k & 1
 			\end{pmatrix}
 			\\
 			&=
 			\begin{pmatrix}
 			x_1^{k-1}-x_1^{k-2}x_k         & x_1^{k-2}-x_1^{k-3}x_k         & \cdots & x_1-x_k     & 1      \\
 			\vdots                         & \vdots                         &        & \vdots      & \vdots \\
 			x_{k-1}^{k-1}-x_{k-1}^{k-2}x_k & x_{k-1}^{k-2}-x_{k-1}^{k-3}x_k & \cdots & x_{k-1}-x_k & 1      \\
 			0                              & 0                              & \cdots & 0           & 1
 			\end{pmatrix}
 			\\
 			&=
 			\begin{pmatrix}
 			(x_1-x_k)x_1^{k-2}             & (x_1-x_k)x_1^{k-3}             & \cdots & x_1-x_k     & 1      \\
 			\vdots                         & \vdots                         &        & \vdots      & \vdots \\
 			(x_{k-1}-x_k)x_{k-1}^{k-2}     & (x_{k-1}-x_k)x_{k-1}^{k-3}     & \cdots & x_{k-1}-x_k & 1      \\
 			0                              & 0                              & \cdots & 0           & 1
 			\end{pmatrix}
 		\end{align*}
 		also ist (Laplacescher Entwicklungssatz nach letzter Zeile):
 		\begin{align*}
 			\det X &= 0+(-1)^{k+k}\det
 			\begin{pmatrix}
 			(x_1-x_k)x_1^{k-2}         & (x_1-x_k)x_1^{k-3}         & \cdots & x_1-x_k     \\
 			\vdots                     & \vdots                     &        & \vdots      \\
 			(x_{k-1}-x_k)x_{k-1}^{k-2} & (x_{k-1}-x_k)x_{k-1}^{k-3} & \cdots & x_{k-1}-x_k \\
 			\end{pmatrix}
 			\\
 			&= 1\cdot(x_1-x_k)\cdots (x_{k-1}-x_k)\det
 			\begin{pmatrix}
 			x_1^{k-2} & \cdots & 1 \\
 			\vdots &  & \vdots \\
 			x_{k-1}^{k-2} & \cdots & 1
 			\end{pmatrix}
 			\\
 			&=(x_1-x_k)\cdots(x_{k-1}-x_k)\prod_{\substack{i<j\\ i,j\in\{1,\dots, k-1 \}}}(x_i-x_j)\\
 			&= \prod_{\substack{i<j\\ i,j\in \{1,\dots,k\}}}(x_i-x_j)
 		\end{align*}
 		Also folgt die Behauptung mit Induktion.
 	\end{Definition}
 \subsection{Fortsetzungssatz für Determinantenformen}
 	\begin{Satz}[Fortsetzungssatz für Determinantenformen]
 		Ist $ (b_i)_{i\in \{1,\dots,n\}} $ eine Basis von $ V $ (also $ \dim V = n $) und $ d\in K $, so gilt:
 		\[
 			\exists! \omega\in\Lambda^nV^*:\omega(b_1,\dots,b_n) = d
 		\]
 	\end{Satz}
 	\paragraph{Beweis}
 		Eindeutigkeit folgt aus der Leibniz-Formel und der Tatsache, dass $ V=[(b_i)_{i\in\{1,\dots,n\}}] $.

 		Existenz: Gegeben sind eine Basis $ (b_i)_{i\in \{1,\dots, n\}} $ von $ V $ und $ d\in K $. Wir definieren $ \omega $ durch die Leibniz-Formel:
 		\[
 			\omega: \overbrace{V\times \dots \times V}^{n\text{-mal}} \to K,\ \omega(v_1,\dots,v_n):=d\cdot\det X
 		\]
 		wobei $ X\in K^{n\times n} $ die Koeffizientenmatrix für die Basisdarstellung der $ (v_i) $ ist,
 		\[
 			(v_1,\dots,v_n)=(b_1,\dots,b_n)\cdot X.
 		\]
 		Dann gilt:
 		\begin{itemize}
 			\item $ \omega $ ist wohldefiniert, da $ (b_i)_{i\in \{i,\dots,n\}} $ linear unabhängig ist, womit die Koeffizienten $ x_{ij},\ i=1,\dots,n $ für jedes $ j=1,\dots,n $ eindeutig sind.
 			\item $ \omega $ ist $ n $-Form, d.h.
 			      \[
 			      	\forall j=1,\dots,n: v_j\mapsto d\cdot\det X
 			      \]
 			      ist linear; offensichtlich!
 			\item $ \omega $ ist alternierend, d.h.
 			      \[
 			      	\omega(v_1,\dots,v_n)=0\text{ falls } v_i = v_j \text{ für }i\neq j;
 			      \]
 			      ist aber $ v_i = v_j $ für ein Paar $ (i,j) $ mit $ i\neq j $, so gilt:
 			      \[
 			      	\sgn(\sigma)x_{\sigma(1)1}\cdots x_{\sigma(n)n} + \sgn(\tau_{ij}\circ \sigma) x_{(\tau_{ij}\circ \sigma)(1)1}\cdots x_{(\tau_{ij}\circ \sigma)(n)n} = 0,
 			      \]
 			      denn
 			      \[
 			      	\sgn(\tau_{ij}\circ \sigma) = -\sgn(\sigma) \quad\text{und}\quad x_{(\tau_{ij}\circ\sigma)(1)1}\cdots x_{(\tau_{ij}\circ\sigma)(n)n} = x_{\sigma(1)1}\cdots x_{\sigma(n)n}
 			      \]
 			      da $ v_i = v_j $.

 			      Damit folgt:
 			      \[
 			      	\sum_{\sigma\in S_n}\sgn(\sigma)x_{\sigma(1)1}\cdots x_{\sigma(n)n}=0,
 			      \]
 			      da mit $ \sigma\in S_n $ auch $ \tau_{ij}\circ\sigma\in S_n $ in der Summe vorkommt.
 		\end{itemize}
 \subsection{Korollar}
 	\begin{Korollar}[Dimension der Determinantenform]
 		Ist $ \dim V = n $, so ist $ \dim\Lambda^nV^* = 1 $. Beweis in der Übung.
 	\end{Korollar}
 \subsection{Korollar}
 	Ist $ \omega(v_1,\dots,v_n)=0 $ für eine Determinantenform $ \omega\in\Lambda^nV^*\setminus\{0\}, $ so ist die Familie $ (v_i)_{i\in \{1,\dots, n\}} $ linear abhängig.
 	\paragraph{Beweis}
 		Ist $ \omega\neq 0 $, so existiert eine Basis $ (b_i)_{i=1,\dots,n} $ von $ V $ mit $ \omega(b_1,\dots,b_n)=d\neq 0 $.
 		Annahme: $ (v_i)_{i\in \{1,\dots,n\}} $ ist linear unabhängig, d.h. $ (v_i) $ ist Basis und damit existiert $ Y\in K^{n\times n} $ mit
 		\[
 			(b_1,\dots,b_n)=(v_1,\dots,v_n)Y,
 		\]
 		nach Leibniz-Formel ist damit
 		\[
 			0\neq \omega(b_1,\dots,b_n)=\omega(v_1,\dots,v_n)\cdot \det Y.
 		\]
 		Damit folgt
 		\[
 			\omega(v_1,\dots,v_n)\neq 0 \text{ (und }\det Y \neq 0)
 		\]

 	\paragraph{Bemerkung}
 		Ist also $ \dim V=n $ und sind $ \omega\in \Lambda^nV^* $ und $ (b_i)_{i\in \{1,\dots,n\}} $ eine Familie in $ V $, so folgt aus zwei der folgenden Aussagen die dritte:
 		\begin{enumerate}[(i)]
 			\item $ \omega\neq 0 $
 			\item $ \omega(b_1,\dots,b_n)\neq 0 $
 			\item $ (b_i)_{i\in\{1,\dots,n\}} $ ist Basis von $ V $.
 		\end{enumerate}
 	\paragraph{Bemerkung}
 		Weiter folgt damit: Sind $ f\in \End(V),\ (b_i)_{i\in \{1,\dots,n\}} $ Basis von $ V $ und $ \omega\in \Lambda^nV^*\setminus\{0\}, $ so gilt:
 		\[
 			f\in \mathrm{Gl}(V)\Leftrightarrow \omega(f(b_1),\dots,f(b_n))\neq 0
 		\]
 		bzw.
 		\[
 			\mathrm{Gl}(V)=\{f\in \End(V)\mid \omega(f(b_1),\dots,f(b_n))\neq 0\}
 		\]
